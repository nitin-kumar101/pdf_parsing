[
  {
    "text": "Log-based software monitoring: a\nsystematic mapping study\nJeanderson Cândido1,2, Maurício Aniche1 and Arie van Deursen1\n1 Department of Software Technology, Delft University of Technology, Delft, Netherlands\n2 Adyen N.V., Amsterdam, Netherlands\nABSTRACT\nModern software development and operations rely on monitoring to understand\nhow systems behave in production. The data provided by application logs and\nruntime environment are essential to detect and diagnose undesired behavior and\nimprove system reliability. However, despite the rich ecosystem around industry-\nready log solutions, monitoring complex systems and getting insights from log data\nremains a challenge. Researchers and practitioners have been actively working to\naddress several challenges related to logs, e.g., how to effectively provide better\ntooling support for logging decisions to developers, how to effectively process and\nstore log data, and how to extract insights from log data.",
    "section_title": "Log-based software monitoring: a systematic mapping study",
    "section_level": 1,
    "page_number": 1,
    "chunk_index": 0,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Log-based software monitoring: a systematic mapping study",
      "section_level": 1
    }
  },
  {
    "text": "A holistic view of the\nresearch effort on logging practices and automated log analysis is key to provide\ndirections and disseminate the state-of-the-art for technology transfer. In this paper,\nwe study 108 papers (72 research track papers, 24 journals, and 12 industry track\npapers) from different communities (e.g., machine learning, software engineering,\nand systems) and structure the research ﬁeld in light of the life-cycle of log data.\nOur analysis shows that (1) logging is challenging not only in open-source projects\nbut also in industry, (2) machine learning is a promising approach to enable a\ncontextual analysis of source code for log recommendation but further investigation\nis required to assess the usability of those tools in practice, (3) few studies approached\nefﬁcient persistence of log data, and (4) there are open opportunities to analyze\napplication logs and to evaluate state-of-the-art log analysis techniques in a DevOps\ncontext.",
    "section_title": "Log-based software monitoring: a systematic mapping study",
    "section_level": 1,
    "page_number": 1,
    "chunk_index": 1,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Log-based software monitoring: a systematic mapping study",
      "section_level": 1
    }
  },
  {
    "text": "Subjects Emerging Technologies, Software Engineering\nKeywords Logging practices, Log infrastructure, Log analysis, DevOps, Monitoring\nINTRODUCTION\nSoftware systems are everywhere and play an important role in society and economy.\nFailures in those systems may harm entire businesses and cause unrecoverable loss in the\nworst case. For instance, in 2018, a supermarket chain in Australia remained closed\nnationwide for 3 h due to “minor IT problems” in their checkout system (Chung, 2018).\nMore recently, in 2019, a misconﬁguration and a bug in a data center management system\ncaused a worldwide outage in the Google Cloud platform, affecting not only Google’s\nservices, but also businesses that use their platform as a service, e.g., Shopify and Snapchat\n(Wired, 2019; Google, 2019).\nWhile software testing plays an important role in preventing failures and assessing\nreliability, developers and operations teams rely on monitoring to understand how the\nHow to cite this article Cândido J, Aniche M, van Deursen A. 2021. . PeerJ\nComput. Sci.",
    "section_title": "Log-based software monitoring: a systematic mapping study",
    "section_level": 1,
    "page_number": 1,
    "chunk_index": 2,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Log-based software monitoring: a systematic mapping study",
      "section_level": 1
    }
  },
  {
    "text": "7:e489 DOI 10.7717/peerj-cs.489\nSubmitted 20 November 2020\nAccepted 22 March 2021\nPublished 6 May 2021\nCorresponding author\nJeanderson Cândido,\nj.candido@tudelft.nl\nAcademic editor\nMarieke Huisman\nAdditional Information and\nDeclarations can be found on\npage 29\nDOI 10.7717/peerj-cs.489\nCopyright\n2021 Cândido et al.\nDistributed under\nCreative Commons CC-BY 4.0\n\n\nsystem behaves in production. In fact, the symbiosis between development and operations\nresulted in a mix known as DevOps (Bass, Weber & Zhu, 2015; Dyck, Penners & Lichter,\n2015; Roche, 2013), where both roles work in a continuous cycle. In addition, given the rich\nnature of data produced by large-scale systems in production and the popularization of\nmachine learning, there is an increasingly trend to adopt artiﬁcial intelligence to automate\noperations. Gartner (2019) refers to this movement as AIOps and also highlights\ncompanies providing automated operations as a service.",
    "section_title": "Log-based software monitoring: a systematic mapping study",
    "section_level": 1,
    "page_number": 1,
    "chunk_index": 3,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Log-based software monitoring: a systematic mapping study",
      "section_level": 1
    }
  },
  {
    "text": "Unsurprisingly, the demand to\nanalyze operations data fostered the creation of a multi-million dollar business\n(TechCrunch, 2017, Investor’s Business Daily, 2018) and plethora of open-source and\ncommercial tools to process and manage log data. For instance, the Elastic stack\n(https://www.elastic.co/what-is/elk-stack) (a.k.a. “ELK” stack) is a popular option to collect,\nprocess, and analyze log data (possibly from different sources) in a centralized manner.\nFigure 1 provides an overview about how the life-cycle of log data relates to different\nstages of the development cycle. First, the developer instruments the source with API\ncalls to a logging framework (e.g., SLF4J or Log4J) to record events about the internal\nstate of the system (in this case, whenever the reference “ data ” is “null”). Once the system\nis live in production, it generates data continuously whenever the execution ﬂow reaches\nthe log statements.",
    "section_title": "Log-based software monitoring: a systematic mapping study",
    "section_level": 1,
    "page_number": 1,
    "chunk_index": 4,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Log-based software monitoring: a systematic mapping study",
      "section_level": 1
    }
  },
  {
    "text": "The data provided by application logs (i.e., data generated from API\ncalls of logging frameworks) and runtime environments (e.g., CPU and disk usage) are\nessential to detect and diagnose undesired behavior and improve software reliability. In\npractice, companies rely on a logging infrastructure to process and manage that data.\nIn the context of the Elastic stack, possible components would be Elasticsearch\n(https://www.elastic.co/elasticsearch/), Logstash (https://www.elastic.co/logstash) and\nKibana (https://www.elastic.co/kibana): Logstash is a log processor tool with several\nplugins available to parse and extract log data, Kibana provides an interface for\nvisualization, query, and exploration of log data, and Elasticsearch, the core component of\nthe Elastic stack, is a distributed and fault-tolerant search engine built on top of Apache\nLucene (https://lucene.apache.org).",
    "section_title": "Log-based software monitoring: a systematic mapping study",
    "section_level": 1,
    "page_number": 1,
    "chunk_index": 5,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Log-based software monitoring: a systematic mapping study",
      "section_level": 1
    }
  },
  {
    "text": "Variants of those components from other vendors include\nGrafana (https://grafana.com) for user interface and Fluentd (https://www.ﬂuentd.org) for log\nLog Infrastructure\nLog Analysis\nLog \nProcessor\nRunning Application\nBuild and Deployment\n…\n…\n…\nLog data\nlog\ngeneration\nExecution Environment\nSource code\nlog\ntransmission\nStorage\nOPs Engineer\nLogging\nDashboards\nDeveloper\n…\n…\nQuery\n…\n…\nFigure 1 Overview of the life-cyle of log data.\nFull-size\n\nDOI: 10.7717/peerj-cs.489/ﬁg-1\nCândido et al. (2021), PeerJ Comput. Sci., DOI 10.7717/peerj-cs.489\n2/38\n\n\nprocessing. Once the data is available, operations engineers use dashboards to analyze trends\nand query the data (“Log Analysis”).\nUnfortunately, despite the rich ecosystem around industry-ready log solutions,\nmonitoring complex systems and getting insights from log data is challenging. For\ninstance, developers need to make several decisions that affect the quality and usefulness of\nlog data, e.g., where to place log statements and what information should be included in the\nlog message.",
    "section_title": "Log-based software monitoring: a systematic mapping study",
    "section_level": 1,
    "page_number": 1,
    "chunk_index": 6,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Log-based software monitoring: a systematic mapping study",
      "section_level": 1
    }
  },
  {
    "text": "In addition, log data can be voluminous and heterogeneous due to how\nindividual developers instrument an application and also the variety in a software stack\nthat compose a system. Those characteristics of log data make it exceedingly hard to\nmake optimal use of log data at scale. In addition, companies need to consider privacy,\nretention policies, and how to effectively get value from data. Even with the support of\nmachine learning and growing adoption of big data platforms, it is challenging to process\nand analyze data in a costly and timely manner.\nThe research community, including practitioners, have been actively working to address\nthe challenges related to the typical life-cycle of log, i.e., how to effectively provide\nbetter tooling support for logging decisions to developers (“Logging”), how to effectively\nprocess and store log data (“Logging Infrastructure”), and how to extract insights from log\ndata (“Log Analysis”). Previously, Rong et al. (2017) conducted a survey involving 41\nprimary studies to understand what was the current state of logging practices.",
    "section_title": "Log-based software monitoring: a systematic mapping study",
    "section_level": 1,
    "page_number": 1,
    "chunk_index": 7,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Log-based software monitoring: a systematic mapping study",
      "section_level": 1
    }
  },
  {
    "text": "They\nfocused their analysis on studies addressing development practices of logging. El-Masri\net al. (2020) conducted an in-depth analysis of 11 log parsing (referred as “log abstraction”)\ntechniques and proposed a quality model based on a survey of 89 primary studies on log\nanalysis and parsing. While these are useful, no overview exists that includes other\nimportant facets of log analysis (e.g., log analysis for quality assurance), connects the\ndifferent log-related areas, and identiﬁes the most pressing open challenges. This\nin-breadth knowledge is key not only to provide directions to the research community\nbut also to bridge the gap between the different research areas, and to summarize the\nliterature for easy access to practitioners.\nIn this paper, we propose a systematic mapping of the logging research area. To that\naim, we study 108 papers that appeared in top-level peer-reviewed conferences and\njournals from different communities (e.g., machine learning, software engineering, and\nsystems).",
    "section_title": "Log-based software monitoring: a systematic mapping study",
    "section_level": 1,
    "page_number": 1,
    "chunk_index": 8,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Log-based software monitoring: a systematic mapping study",
      "section_level": 1
    }
  },
  {
    "text": "We structure the research ﬁeld in light of the life-cycle of log data, elaborate the\nfocus of each research area, and discuss opportunities and directions for future work.\nOur analysis shows that (1) logging is a challenge not only in open-source projects but also\nin industry, (2) machine learning is a promising approach to enable contextual analysis of\nsource code for log recommendations but further investigation is required to assess the\nusability of those tools in practice, (3) few studies address efﬁcient persistence of log data,\nand (4) while log analysis is mature ﬁeld with several applications (e.g., quality assurance\nand failure prediction), there are open opportunities to analyze application logs and to\nevaluate state-of-the-art techniques in a DevOps context.\nCândido et al. (2021), PeerJ Comput. Sci., DOI 10.7717/peerj-cs.489\n3/38\n\n\nSURVEY METHODOLOGY\nThe goal of this paper is to discover, categorize, and summarize the key research results in\nlog-based software monitoring.",
    "section_title": "Log-based software monitoring: a systematic mapping study",
    "section_level": 1,
    "page_number": 1,
    "chunk_index": 9,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Log-based software monitoring: a systematic mapping study",
      "section_level": 1
    }
  },
  {
    "text": "To this end, we perform a systematic mapping study to\nprovide a holistic view of the literature in logging and automated log analysis. Concretely,\nwe investigate the following research questions:\n\u0001 RQ1: What are the publication trends in research on log-based monitoring over the years?\n\u0001 RQ2: What are the different research scopes of log-based monitoring?\nThe ﬁrst research question (RQ1) addresses the historical growth of the research ﬁeld.\nAnswering this research question enables us to identify the popular venues and the\ncommunities (e.g., Software Engineering, Distributed Systems) that have been focusing\non log-based monitoring innovation. Furthermore, we aim at investigating the participation\nof industry in the research ﬁeld. Researchers can beneﬁt from our analysis by helping them to\nmake a more informed decision regarding venues for paper submission. In addition, our\nanalysis also serves as a guide to practitioners willing to engage with the research community\neither by attending conferences or looking for references to study and experiment.",
    "section_title": "Log-based software monitoring: a systematic mapping study",
    "section_level": 1,
    "page_number": 1,
    "chunk_index": 10,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Log-based software monitoring: a systematic mapping study",
      "section_level": 1
    }
  },
  {
    "text": "The\nsecond research question (RQ2) addresses the actual mapping of the primary studies.\nAs illustrated in Fig. 1, the life-cycle of log data contains different inter-connected contexts\n(i.e., “Logging”, “Log Infrastructure”, and “Log Analysis”) with their own challenges and\nconcerns that span the entire development cycle. Answering this research question enables\nus to identify those concerns for each context and quantify the research effort by the number\nof primary studies in a particular category. In addition, we aim at providing an overview\nof the studies so practitioners and researchers are able to use our mapping study as a starting\npoint for an in-depth analysis of a particular topic of interest.\nOverall, we follow the standard guidelines for systematic mapping (Petersen et al., 2008).\nOur survey methodology is divided into four parts as illustrated in Fig. 2. First, we perform\npreliminary searches to derive our search criteria and build an initial list of potential\nrelevant studies based on ﬁve data sources.",
    "section_title": "Log-based software monitoring: a systematic mapping study",
    "section_level": 1,
    "page_number": 1,
    "chunk_index": 11,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Log-based software monitoring: a systematic mapping study",
      "section_level": 1
    }
  },
  {
    "text": "Next, we apply our inclusion/exclusion criteria\nto arrive at the eventual list of selected papers up to 2018 (when we ﬁrst conducted the\nsurvey). We then conduct the data extraction and classiﬁcation procedures. Finally, we\nupdate the results of our survey to include papers published in 2019.\nSearch Process\nStudy Selection\nsearch\nData\nsources\nQuery\nstrings\nearch Proce\nrefinement\nGoogle Scholar, \nACM, IEEE, \nScopus, Springer\nData\nurces\nInitial list\n(4,187)\nSelection\n(96)\nt\nS\nPreliminary list\n(315)\nstep 1\nselection criteria\nstep 2\nrank criteria\nClassification \nSchema\nsort\nn\nAbstracts\nPapers\nkeywording\nupdate\nClassification\nstep 1\nSurvey Update\ns\n'18 Papers\n(11)\n'19 Papers\n(12)\nfwd. snowballing\nFigure 2 Overview of survey methodology: our four steps consists of the discovery of related studies (“Search Process”), the selection of\nrelevant studies (“Study Selection”), the mapping process (“Classiﬁcation”), and the update for papers published in 2019 (“Survey\nUpdate”).\nFull-size\n\nDOI: 10.7717/peerj-cs.489/ﬁg-2\nCândido et al. (2021), PeerJ Comput.",
    "section_title": "Log-based software monitoring: a systematic mapping study",
    "section_level": 1,
    "page_number": 1,
    "chunk_index": 12,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Log-based software monitoring: a systematic mapping study",
      "section_level": 1
    }
  },
  {
    "text": "Sci., DOI 10.7717/peerj-cs.489\n4/38\n\n\nData sources and search process\nTo conduct our study, we considered ﬁve popular digital libraries from different publishers\nbased on other literature reviews in software engineering, namely, ACM Digital Library,\nIEEE Xplore, SpringerLink, Scopus, and Google Scholar. By considering ﬁve digital\nlibraries, we maximize the range of venues and increase the diversity of studies related\nto logging. In addition, this decision reduces the bias caused by the underlying search\nengine since two digital libraries may rank the results in a different way for the same\nequivalent search.\nWe aim to discover relevant papers from different areas as much as possible. However,\nit is a challenge to build an effective query for the ﬁve selected digital libraries without\ndealing with a massive amount of unrelated results, since terms such as “log” and “log\nanalysis” are pervasive in many areas. Conversely, inﬂating the search query with\nspeciﬁc terms to reduce false positives would bias our study to a speciﬁc context (e.g., log\nanalysis for debugging).",
    "section_title": "Log-based software monitoring: a systematic mapping study",
    "section_level": 1,
    "page_number": 1,
    "chunk_index": 13,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Log-based software monitoring: a systematic mapping study",
      "section_level": 1
    }
  },
  {
    "text": "To ﬁnd a balance between those cases, we conducted preliminary\nsearches with different terms and search scopes, e.g., full text, title, and abstract. We\nconsidered terms based on “log”, its synonyms, and activities related to log analysis.\nDuring this process, we observed that forcing the presence of the term “log” helps to order\nrelevant studies on the ﬁrst pages. In case the data source is unable to handle word\nstemming automatically (e.g., “log” and “logging”), we enhance the query with the\nkeywords variations. In addition, conﬁgured the data sources to search on titles and\nabstracts whenever it was possible. In case the data source provides no support to search on\ntitles and abstracts, we considered only titles to reduce false positives. This process resulted\nin the following search query:\nlog AND (trace OR event OR software OR system OR code OR detect OR mining OR\nanalysis OR monitoring OR web OR technique OR develop OR pattern OR practice)\nDealing with multiple libraries requires additional work to merge data and remove\nduplicates.",
    "section_title": "Log-based software monitoring: a systematic mapping study",
    "section_level": 1,
    "page_number": 1,
    "chunk_index": 14,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Log-based software monitoring: a systematic mapping study",
      "section_level": 1
    }
  },
  {
    "text": "In some cases, the underlying information retrieval algorithms yielded\nunexpected results when querying some libraries, such as duplicates within the data source\nand entries that mismatch the search constraints. To overcome those barriers, we\nimplemented auxiliary scripts to cleanup the dataset. We index the entries by title to\neliminate duplicates, and we remove entries that fail to match the search criteria.\nFurthermore, we keep the most recent work when we identify two entries with the same\ntitle and different publication date (e.g., journal extension from previous work).\nAs of December of 2018, when we ﬁrst conducted this search, we extracted 992 entries\nfrom Google Scholar, 1,122 entries from ACM Digital Library, 1,900 entries from IEEE\nXplore, 2,588 entries from Scopus, and 7,895 entries from SpringerLink (total of 14,497\nentries). After merging and cleaning the data, we ended up with 4,187 papers in our initial\nlist.\nStudy selection\nWe conduct the selection process by assessing the 4,187 entries according to inclusion/\nexclusion criteria and by selecting publications from highly ranked venues.",
    "section_title": "Log-based software monitoring: a systematic mapping study",
    "section_level": 1,
    "page_number": 1,
    "chunk_index": 15,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Log-based software monitoring: a systematic mapping study",
      "section_level": 1
    }
  },
  {
    "text": "We deﬁne the\ncriteria as follows:\nCândido et al. (2021), PeerJ Comput. Sci., DOI 10.7717/peerj-cs.489\n5/38\n\n\n\u0001 C1: It is an English manuscript.\n\u0001 C2: It is a primary study.\n\u0001 C3: It is a full research paper accepted through peer-review.\n\u0001 C4: The paper uses the term “log” in a software engineering context, i.e., logs to describe\nthe behavior of a software system. We exclude papers that use the term “log” in an\nunrelated semantic (e.g., deforestation, life logging, well logging, log function).\nThe rationale for criterion C1 is that major venues use English as the standard idiom for\nsubmission. The rationale for criterion C2 is to avoid including secondary studies in our\nmapping, as suggested by Kitchenham & Charters (2007). In addition, the process of\napplying this criterion allows us to identify other systematic mappings and systematic\nliterature reviews related to ours. The rationale for criterion C3 is that some databases\nreturn gray literature as well as short papers; our focus is on full peer-reviewed research\npapers, which we consider mature research, ready for real-world tests.",
    "section_title": "Log-based software monitoring: a systematic mapping study",
    "section_level": 1,
    "page_number": 1,
    "chunk_index": 16,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Log-based software monitoring: a systematic mapping study",
      "section_level": 1
    }
  },
  {
    "text": "Note that different\nvenues might have different page number speciﬁcations to determine whether a\nsubmission is a full or short paper, and these speciﬁcations might change over time.\nWe consulted the page number from each venue to avoid unfair exclusion. The rationale\nfor criterion C4 is to exclude papers that are unrelated to the scope of this mapping\nstudy. We noticed that some of the results are in the context of, e.g., mathematics and\nenvironmental studies. While we could have tweaked our search criteria to minimize\nthe occurrence of those false positives (e.g., NOT deforestation), we were unable to\nsystematically derive all keywords to exclude; therefore, we favored higher false positive\nrate in exchange of increasing the chances of discovering relevant papers.\nThe ﬁrst author manually performed the inclusion procedure. He analyzed the title and\nabstracts of all the papers marking the paper as “in” or “out”. During this process, the\nauthor applied the criteria and categorized the reasons for exclusion.",
    "section_title": "Log-based software monitoring: a systematic mapping study",
    "section_level": 1,
    "page_number": 1,
    "chunk_index": 17,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Log-based software monitoring: a systematic mapping study",
      "section_level": 1
    }
  },
  {
    "text": "For instance,\nwhenever an entry fails the criteria C4, the authors classiﬁed it as “Out of Scope”. The\ncategories we used are: “Out of Scope”, “Short/workshop paper”, “Not a research paper”,\n“Unpublished” (e.g., unpublished self-archived paper indexed by Google Scholar),\n“Secondary study”, and “Non-English manuscript”. It is worth mentioning that we ﬂagged\nthree entries as “Duplicate” as our merging step missed these cases due to special\ncharacters in the title. After applying the selection criteria, we removed 3,872 entries\nresulting in 315 entries.\nIn order to ﬁlter the remaining 315 papers by rank, we used the CORE Conference\nRank (CORE Rank) (http://www.core.edu.au/conference-portal) as a reference. We\nconsidered studies published only in venues ranked as A* or A. According to the CORE\nRank, those categories indicate that the venue is widely known in the computer science\ncommunity and has a strict review process by experienced researches. After applying the\nrank criteria, we removed 219 papers.",
    "section_title": "Log-based software monitoring: a systematic mapping study",
    "section_level": 1,
    "page_number": 1,
    "chunk_index": 18,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Log-based software monitoring: a systematic mapping study",
      "section_level": 1
    }
  },
  {
    "text": "Our selection consists of (315 −219 =) 96 papers after applying inclusion/exclusion\ncriteria (step 1) and ﬁltering by venue rank (step 2). Table 1 summarises the selection\nprocess.\nCândido et al. (2021), PeerJ Comput. Sci., DOI 10.7717/peerj-cs.489\n6/38\n\n\nData extraction and classification\nWe focus the data extraction process to the required data to answer our research\nquestions.\nTo answer RQ1, we collect metadata from the papers and their related venues.\nConcretely, we deﬁne the following schema: “Year of publication”, “Type of publication”,\n“Venue name”, and “Research community”. The ﬁelds “Year of publication” and\n“Venue name” are readly available on the scrapped data from the data sources. To extract\nthe ﬁeld “Type of publication”, we automatically assign the label “journal” if it is a\njournal paper. For conference papers, we manually check the proceedings to determine\nif it is a “research track” or “industry track” paper (we assume “research track” if not\nexplicitly stated). To extract the ﬁeld “Research community”, we check the topics of\ninterest from the conferences and journals.",
    "section_title": "Log-based software monitoring: a systematic mapping study",
    "section_level": 1,
    "page_number": 1,
    "chunk_index": 19,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Log-based software monitoring: a systematic mapping study",
      "section_level": 1
    }
  },
  {
    "text": "This information is usually available in a “call\nfor papers” page. Later, we manually aggregate the venues and we merge closely related\ntopics (e.g., Artiﬁcial Intelligence, Machine Learning, and Data Science). While a\nTable 1 Distribution of study selection when the survey was ﬁrst conducted.\nSelection Step\nQty\nStep 1. Exclusion by selection criteria\n3,872\nOut of scope (failed C4)\n3,544\nShort/workshop paper (failed C3)\n276\nNot a research paper (failed C3)\n40\nNon-English manuscript (failed C1)\n4\nUnpublished (failed C3)\n3\nDuplicate\n3\nSecondary study (failed C2)\n2\nPreliminary inclusion of papers\n315\nStep 2. Exclusion by venue rank (neither A* nor A)\n219\nUnranked\n143\nRank B\n47\nRank C\n30\nInclusion of papers (up to 2018, inclusive)\n96\nextract\nTopics of \nInterest\nConference\nProceedings\nce\ngs\nCall for\nPapers\nResearch community\nextract\nretrieve\nextract\nPrimary\nStudy\nYear of publication\nType of publication\nResearch track, Industry track, Journal\nVenue name\nextract\nrefinement\nFigure 3 Data extraction for RQ1.\nFull-size\n\nDOI: 10.7717/peerj-cs.489/ﬁg-3\nCândido et al. (2021), PeerJ Comput.",
    "section_title": "Log-based software monitoring: a systematic mapping study",
    "section_level": 1,
    "page_number": 1,
    "chunk_index": 20,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Log-based software monitoring: a systematic mapping study",
      "section_level": 1
    }
  },
  {
    "text": "Sci., DOI 10.7717/peerj-cs.489\n7/38\n\n\ncomplete meta-analysis is out of scope from our study, we believe the extracted data is\nsufﬁcient to address the research question. Figure 3 summarizes the process for RQ1.\nTo answer RQ2, we collect the abstracts from the primary studies. In this process, we\nstructure the abstract to better identify the motivation of the study, what problem the\nauthors are addressing, how the researchers are mitigating the problem, and the results of\nthe study. Given the diverse set of problems and domains, we ﬁrst group the studies\naccording to their overall context (e.g., whether the paper relates to “Logging”, “Log\nInfrastructure”, or “Log Analysis”). To mitigate self-bias, we conducted two independent\ntriages and compared our results. In case of divergence, we review the paper in depth to\nassign the context that better ﬁts the paper. To derive the classiﬁcation schemafor each\ncontext, we perform the keywording of abstracts (Petersen et al., 2008).",
    "section_title": "Log-based software monitoring: a systematic mapping study",
    "section_level": 1,
    "page_number": 1,
    "chunk_index": 21,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Log-based software monitoring: a systematic mapping study",
      "section_level": 1
    }
  },
  {
    "text": "In this process,\nwe extract keywords in the abstract (or introduction, if necessary) and cluster similar\nkeywords to create categories. We perform this process using a random sample of papers\nto derive an initial classiﬁcation schema.\nLater, with all the papers initially classiﬁed, the authors explored the speciﬁc objectives\nof each paper and review the assigned category. To that aim, the ﬁrst and second authors\nperformed card sorting (Spencer & Warfel, 2004; Usability.gov, 2019) to determine the\ngoal of each of the studied papers. Note that, in case new categories emerge in this process,\nwe generalize them in either one of the existing categories or enhance our classiﬁcation\nschema to update our view of different objectives in a particular research area. After the\nﬁrst round of card sorting, we noticed that some of the groups (often the ones with high\nnumber of papers) could be further broken down in subcategories (we discuss the\ncategories and related subcategories in the Results section).",
    "section_title": "Log-based software monitoring: a systematic mapping study",
    "section_level": 1,
    "page_number": 1,
    "chunk_index": 22,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Log-based software monitoring: a systematic mapping study",
      "section_level": 1
    }
  },
  {
    "text": "The ﬁrst author conducted two separate blinded classiﬁcations on different periods of\ntime to measure the degree of adherence to the schema given that classiﬁcation is subject\nof interpretation, and thus, a source of bias. The same outcome converged on 83% of\nthe cases (80 out of the 96 identiﬁed papers). The divergences were then discussed with the\nsecond author of this paper. Furthermore, the second author reviewed the resulting\nclassiﬁcation. Note that, while a paper may address more than one category, we choose the\nLogging, Log Infrastructure, Log Analysis\nclustering\nClassification schema\nAbstract\nextract\nStructured\nAbstract\nPaper context\ntriage\nextract\nKeywords\nupdate\nreview\nPrimary\nStudies\nFigure 4 Data extraction and classiﬁcation for RQ2. The dashed arrows denote the use of the data\nschema by the researchers with the primary studies.\nFull-size\n\nDOI: 10.7717/peerj-cs.489/ﬁg-4\nCândido et al. (2021), PeerJ Comput. Sci., DOI 10.7717/peerj-cs.489\n8/38\n\n\ncategory related to the most signiﬁcant contribution of that paper. Figure 4 summarizes the\nprocess for RQ2.",
    "section_title": "Log-based software monitoring: a systematic mapping study",
    "section_level": 1,
    "page_number": 1,
    "chunk_index": 23,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Log-based software monitoring: a systematic mapping study",
      "section_level": 1
    }
  },
  {
    "text": "Survey update\nAs of October of 2020, we updated our survey to include papers published in 2019 since\nwe ﬁrst conducted this analysis during December in 2018. To this end, we select all 11\npapers from 2018 and perform forward snowballing to fetch a preliminary list of papers\nfrom 2019. We use snowballing for simplicity since we can leverage the “Cited By” feature\nfrom Google Scholar rather than scraping data of all ﬁve digital libraries. It is worth\nmentioning that we limit the results up to 2019 to avoid incomplete results for 2020.\nFor the preliminary list of 2019, we apply the same selection and rank criteria\n(see Section “Study Selection”); then, we analyze and map the studies according to the\nexisting classiﬁcation schema (see Section “Data Extraction and Classiﬁcation”). In this\nprocess, we identify 12 new papers and merge them with our existing dataset. Our ﬁnal\ndataset consists of (96 + 12 =) 108 papers.\nRESULTS\nPublication trends (RQ1)\nFigure 5 highlights the growth of publication from 1992 to 2019. The interest on logging\nhas been continuously increasing since the early 2000’s.",
    "section_title": "Log-based software monitoring: a systematic mapping study",
    "section_level": 1,
    "page_number": 1,
    "chunk_index": 24,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Log-based software monitoring: a systematic mapping study",
      "section_level": 1
    }
  },
  {
    "text": "During this time span, we\nobserved the appearance of industry track papers reporting applied research in a real\ncontext. This gives some evidence that the growing interest on the topic attracted not only\nresearchers from different areas but also companies, fostering the collaboration between\nacademia and industry.\nWe identiﬁed 108 papers (72 research track papers, 24 journals, and 12 industry track\npapers) published in 46 highly ranked venues spanning different communities (Table 2).\nTable 2 highlights the distribution of venues grouped by the research community,\ne.g., there are 44 papers published on 10 Software Engineering venues.\n1\n1\n1\n2\n1\n2\n1\n1\n1\n3\n1\n1\n5\n1\n6\n3\n1\n5\n2\n4\n2\n8\n1\n4\n2\n2\n6\n1\n3\n6\n5\n2\n5\n5\n1\n8\n3\n1\n'92\n'98\n'07\n'03 '04 '05 '06\n'08\n'10\n'09\n'12\n'11\n'13\n'15 '16\n'14\n'18\n'00\n'17\n'19\nYear\nPublication Type\nindustry track paper\njournal\nresearch track paper\nFigure 5 Growth of publication types over the years. Labels indicate the number of publication per\ntype in a speciﬁc year. There are 108 papers in total.\nFull-size\n\nDOI: 10.7717/peerj-cs.489/ﬁg-5\nCândido et al. (2021), PeerJ Comput.",
    "section_title": "Log-based software monitoring: a systematic mapping study",
    "section_level": 1,
    "page_number": 1,
    "chunk_index": 25,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Log-based software monitoring: a systematic mapping study",
      "section_level": 1
    }
  },
  {
    "text": "Sci., DOI 10.7717/peerj-cs.489\n9/38\n\n\nTable 3 highlights the most recurring venues in our dataset (we omitted venues with less\nthan three papers for brevity). The “International Conference on Software Engineering\n(ICSE)”, the “Empirical Software Engineering Journal (EMSE)”, and the “International\nConference on Dependable Systems and Networks (DSN)” are the top three recurring\nvenues related to the subject and are well-established venues. DSN and ICSE are\nconferences with more than 40 editions each and EMSE is a journal with an average of\nﬁve issues per year since 1996. At a glance, we noticed that papers from DSN have an\nemphasis on log analysis of system logs while papers from ICSE and EMSE have an\nemphasis on development aspects of logging practices (more details about the research\nareas in the next section). Note that Table 3 also concentrates 65% (71 out of 108) of the\nprimary studies in our dataset.\nOverview of research areas (RQ2)\nWe grouped the studied papers among the following three categories based in our\nunderstanding about the life-cycle of log data (see Fig. 1).",
    "section_title": "Log-based software monitoring: a systematic mapping study",
    "section_level": 1,
    "page_number": 1,
    "chunk_index": 26,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Log-based software monitoring: a systematic mapping study",
      "section_level": 1
    }
  },
  {
    "text": "For each category, we derived\nsubcatories that emerged from our keywording process (see Section “Data Extraction and\nClassiﬁcation”):\n\u0001 LOGGING: Research in this category aims at understanding how developers conduct\nlogging practices and providing better tooling support to developers. There are\nthree subcategories in this line of work: (1) empirical studies on logging practices,\n(2) requirements for application logs, and (3) implementation of log statements\n(e.g., where and how to log).\n\u0001 LOG INFRASTRUCTURE: Research in this category aims at improving log processing and\npersistence. There are two subcategories in this line of work: (1) log parsing, and (2) log\nstorage.\n\u0001 LOG ANALYSIS: Research in this category aims at extracting knowledge from log data.\nThere are eight subcategories in this line of work: (1) anomaly detection, (2) security and\nprivacy, (3) root cause analysis, (4) failure prediction, (5) quality assurance, (6) model\ninference and invariant mining, (7) reliability and dependability, and (8) log platforms.\nTable 2 Distribution of venues and publications grouped by research communities.",
    "section_title": "Log-based software monitoring: a systematic mapping study",
    "section_level": 1,
    "page_number": 1,
    "chunk_index": 27,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Log-based software monitoring: a systematic mapping study",
      "section_level": 1
    }
  },
  {
    "text": "Research community\n# of venues\n# of papers\nSoftware Engineering\n10\n44\nDistributed Systems and Cloud Computing\n10\n20\nSystems\n9\n17\nArtiﬁcial Intelligence, Machine Learning, and Data Science (AI)\n8\n13\nSecurity\n5\n7\nInformation Systems\n3\n6\nDatabases\n1\n1\nTotal\n46\n108\nCândido et al. (2021), PeerJ Comput. Sci., DOI 10.7717/peerj-cs.489\n10/38\n\n\nWe provide an overview of the categories, their respective descriptions, and summary of\nour results in Table 4. In summary, we observed that LOG ANALYSIS dominates most of the\nresearch effort (68 out of 108 papers) with papers published since the early 90’s. LOG\nINFRASTRUCTURE is younger than LOG ANALYSIS as we observed papers starting from 2007\n(16 out of 108 papers). LOGGING is the youngest area of interest with an increasing\nmomentum for research (24 out of 108 papers). In the following, we elaborate our analysis\nand provide an overview of the primary studies.\nLogging\nLog messages are usually in the form of free text and may expose parts of the system\nstate (e.g., exceptions and variable values) to provide additional context.",
    "section_title": "Log-based software monitoring: a systematic mapping study",
    "section_level": 1,
    "page_number": 1,
    "chunk_index": 28,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Log-based software monitoring: a systematic mapping study",
      "section_level": 1
    }
  },
  {
    "text": "The full log\nstatement also includes a severity level to indicate the purpose of that statement. Logging\nTable 3 Top recurring venues ordered by number of papers. There are 14 (out of 46) recurring venues with at least three papers published\n(omitted venues with less than three papers for brevity).\nVenue (acronym)\nReferences\nQty\nInternational Conference on Software Engineering\n(ICSE)\nAndrews & Zhang (2003), Yuan, Park & Zhou (2012), Beschastnikh et al. (2014), Fu et al.\n(2014a), Pecchia et al. (2015), Zhu et al. (2015), Lin et al. (2016), Chen & Jiang (2017a), Li\net al. (2019b), Zhu et al. (2019)\n10\nEmpirical Software Engineering Journal (EMSE)\nHuynh & Miller (2009), Shang, Nagappan & Hassan (2015), Russo, Succi & Pedrycz (2015),\nChen & Jiang (2017b), Li, Shang & Hassan (2017), Hassani et al. (2018), Li et al. (2018),\nZeng et al. (2019), Li et al. (2019a)\n9\nIEEE/IFIP International Conference on\nDependable Systems and Networks (DSN)\nOliner & Stearley (2007), Lim, Singh & Yajnik (2008), Cinque et al. (2010), Di Martino,\nCinque & Cotroneo (2012), El-Sayed & Schroeder (2013), Oprea et al.",
    "section_title": "Log-based software monitoring: a systematic mapping study",
    "section_level": 1,
    "page_number": 1,
    "chunk_index": 29,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Log-based software monitoring: a systematic mapping study",
      "section_level": 1
    }
  },
  {
    "text": "(2015), He et al.\n(2016a), Neves, Machado & Pereira (2018)\n8\nInternational Symposium on Software Reliability\nEngineering (ISSRE)\nTang & Iyer (1992), Mariani & Pastore (2008), Banerjee, Srikanth & Cukic (2010), Pecchia &\nRusso (2012), Farshchi et al. (2015), He et al. (2016b), Bertero et al. (2017)\n7\nInternational Conference on Automated Software\nEngineering (ASE)\nAndrews (1998), Chen et al. (2018), He et al. (2018a), Ren et al. (2019), Liu et al. (2019a)\n5\nInternational Symposium on Reliable Distributed\nSystems (SRDS)\nZhou et al. (2010), Kc & Gu (2011), Fu et al. (2012), Chuah et al. (2013), Gurumdimma et al.\n(2016)\n5\nACM International Conference on Knowledge\nDiscovery and Data Mining (KDD)\nMakanju, Zincir-Heywood & Milios (2009), Nandi et al. (2016), Wu, Anchuri & Li (2017), Li\net al. (2017)\n4\nIEEE International Symposium on Cluster, Cloud\nand Grid Computing (CCGrid)\nPrewett (2005), Yoon & Squicciarini (2014), Lin et al. (2015), Di et al. (2017)\n4\nIEEE Transactions on Software Engineering (TSE) Andrews & Zhang (2003), Tian, Rudraraju & Li (2004), Cinque, Cotroneo & Pecchia (2013),\nLiu et al.",
    "section_title": "Log-based software monitoring: a systematic mapping study",
    "section_level": 1,
    "page_number": 1,
    "chunk_index": 30,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Log-based software monitoring: a systematic mapping study",
      "section_level": 1
    }
  },
  {
    "text": "(2019b)\n4\nAnnual Computer Security Applications\nConference (ACSAC)\nAbad et al. (2003), Barse & Jonsson (2004), Yen et al. (2013)\n3\nIBM Journal of Research and Development\nAharoni et al. (2011), Ramakrishna et al. (2017), Wang et al. (2017)\n3\nInternational Conference on Software\nMaintenance and Evolution (ICSME)\nShang et al. (2014), Zhi et al. (2019), Anu et al. (2019)\n3\nIEEE International Conference on Data Mining\n(ICDM)\nFu et al. (2009), Xu et al. (2009a), Tang & Li (2010)\n3\nJournal of Systems and Software (JSS)\nMavridis & Karatza (2017), Bao et al. (2018), Farshchi et al. (2018)\n3\nTotal\n71\nCândido et al. (2021), PeerJ Comput. Sci., DOI 10.7717/peerj-cs.489\n11/38",
    "section_title": "Log-based software monitoring: a systematic mapping study",
    "section_level": 1,
    "page_number": 1,
    "chunk_index": 31,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Log-based software monitoring: a systematic mapping study",
      "section_level": 1
    }
  },
  {
    "text": "Log-based software monitoring: a\nsystematic mapping study\nJeanderson Cândido1,2, Maurício Aniche1 and Arie van Deursen1\n1 Department of Software Technology, Delft University of Technology, Delft, Netherlands\n2 Adyen N.V., Amsterdam, Netherlands\nABSTRACT\nModern software development and operations rely on monitoring to understand\nhow systems behave in production. The data provided by application logs and\nruntime environment are essential to detect and diagnose undesired behavior and\nimprove system reliability. However, despite the rich ecosystem around industry-\nready log solutions, monitoring complex systems and getting insights from log data\nremains a challenge. Researchers and practitioners have been actively working to\naddress several challenges related to logs, e.g., how to effectively provide better\ntooling support for logging decisions to developers, how to effectively process and\nstore log data, and how to extract insights from log data.",
    "section_title": "Introduction",
    "section_level": 2,
    "page_number": 1,
    "chunk_index": 0,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Introduction",
      "section_level": 2
    }
  },
  {
    "text": "A holistic view of the\nresearch effort on logging practices and automated log analysis is key to provide\ndirections and disseminate the state-of-the-art for technology transfer. In this paper,\nwe study 108 papers (72 research track papers, 24 journals, and 12 industry track\npapers) from different communities (e.g., machine learning, software engineering,\nand systems) and structure the research ﬁeld in light of the life-cycle of log data.\nOur analysis shows that (1) logging is challenging not only in open-source projects\nbut also in industry, (2) machine learning is a promising approach to enable a\ncontextual analysis of source code for log recommendation but further investigation\nis required to assess the usability of those tools in practice, (3) few studies approached\nefﬁcient persistence of log data, and (4) there are open opportunities to analyze\napplication logs and to evaluate state-of-the-art log analysis techniques in a DevOps\ncontext.",
    "section_title": "Introduction",
    "section_level": 2,
    "page_number": 1,
    "chunk_index": 1,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Introduction",
      "section_level": 2
    }
  },
  {
    "text": "Subjects Emerging Technologies, Software Engineering\nKeywords Logging practices, Log infrastructure, Log analysis, DevOps, Monitoring\nINTRODUCTION\nSoftware systems are everywhere and play an important role in society and economy.\nFailures in those systems may harm entire businesses and cause unrecoverable loss in the\nworst case. For instance, in 2018, a supermarket chain in Australia remained closed\nnationwide for 3 h due to “minor IT problems” in their checkout system (Chung, 2018).\nMore recently, in 2019, a misconﬁguration and a bug in a data center management system\ncaused a worldwide outage in the Google Cloud platform, affecting not only Google’s\nservices, but also businesses that use their platform as a service, e.g., Shopify and Snapchat\n(Wired, 2019; Google, 2019).\nWhile software testing plays an important role in preventing failures and assessing\nreliability, developers and operations teams rely on monitoring to understand how the\nHow to cite this article Cândido J, Aniche M, van Deursen A. 2021. Log-based software monitoring: a systematic mapping study. PeerJ\nComput. Sci.",
    "section_title": "Introduction",
    "section_level": 2,
    "page_number": 1,
    "chunk_index": 2,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Introduction",
      "section_level": 2
    }
  },
  {
    "text": "7:e489 DOI 10.7717/peerj-cs.489\nSubmitted 20 November 2020\nAccepted 22 March 2021\nPublished 6 May 2021\nCorresponding author\nJeanderson Cândido,\nj.candido@tudelft.nl\nAcademic editor\nMarieke Huisman\nAdditional Information and\nDeclarations can be found on\npage 29\nDOI 10.7717/peerj-cs.489\nCopyright\n2021 Cândido et al.\nDistributed under\nCreative Commons CC-BY 4.0\n\n\nsystem behaves in production. In fact, the symbiosis between development and operations\nresulted in a mix known as DevOps (Bass, Weber & Zhu, 2015; Dyck, Penners & Lichter,\n2015; Roche, 2013), where both roles work in a continuous cycle. In addition, given the rich\nnature of data produced by large-scale systems in production and the popularization of\nmachine learning, there is an increasingly trend to adopt artiﬁcial intelligence to automate\noperations. Gartner (2019) refers to this movement as AIOps and also highlights\ncompanies providing automated operations as a service.",
    "section_title": "Introduction",
    "section_level": 2,
    "page_number": 1,
    "chunk_index": 3,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Introduction",
      "section_level": 2
    }
  },
  {
    "text": "Unsurprisingly, the demand to\nanalyze operations data fostered the creation of a multi-million dollar business\n(TechCrunch, 2017, Investor’s Business Daily, 2018) and plethora of open-source and\ncommercial tools to process and manage log data. For instance, the Elastic stack\n(https://www.elastic.co/what-is/elk-stack) (a.k.a. “ELK” stack) is a popular option to collect,\nprocess, and analyze log data (possibly from different sources) in a centralized manner.\nFigure 1 provides an overview about how the life-cycle of log data relates to different\nstages of the development cycle. First, the developer instruments the source with API\ncalls to a logging framework (e.g., SLF4J or Log4J) to record events about the internal\nstate of the system (in this case, whenever the reference “ data ” is “null”). Once the system\nis live in production, it generates data continuously whenever the execution ﬂow reaches\nthe log statements.",
    "section_title": "Introduction",
    "section_level": 2,
    "page_number": 1,
    "chunk_index": 4,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Introduction",
      "section_level": 2
    }
  },
  {
    "text": "The data provided by application logs (i.e., data generated from API\ncalls of logging frameworks) and runtime environments (e.g., CPU and disk usage) are\nessential to detect and diagnose undesired behavior and improve software reliability. In\npractice, companies rely on a logging infrastructure to process and manage that data.\nIn the context of the Elastic stack, possible components would be Elasticsearch\n(https://www.elastic.co/elasticsearch/), Logstash (https://www.elastic.co/logstash) and\nKibana (https://www.elastic.co/kibana): Logstash is a log processor tool with several\nplugins available to parse and extract log data, Kibana provides an interface for\nvisualization, query, and exploration of log data, and Elasticsearch, the core component of\nthe Elastic stack, is a distributed and fault-tolerant search engine built on top of Apache\nLucene (https://lucene.apache.org).",
    "section_title": "Introduction",
    "section_level": 2,
    "page_number": 1,
    "chunk_index": 5,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Introduction",
      "section_level": 2
    }
  },
  {
    "text": "Variants of those components from other vendors include\nGrafana (https://grafana.com) for user interface and Fluentd (https://www.ﬂuentd.org) for log\nLog Infrastructure\nLog Analysis\nLog \nProcessor\nRunning Application\nBuild and Deployment\n…\n…\n…\nLog data\nlog\ngeneration\nExecution Environment\nSource code\nlog\ntransmission\nStorage\nOPs Engineer\nLogging\nDashboards\nDeveloper\n…\n…\nQuery\n…\n…\nFigure 1 Overview of the life-cyle of log data.\nFull-size\n\nDOI: 10.7717/peerj-cs.489/ﬁg-1\nCândido et al. (2021), PeerJ Comput. Sci., DOI 10.7717/peerj-cs.489\n2/38\n\n\nprocessing. Once the data is available, operations engineers use dashboards to analyze trends\nand query the data (“Log Analysis”).\nUnfortunately, despite the rich ecosystem around industry-ready log solutions,\nmonitoring complex systems and getting insights from log data is challenging. For\ninstance, developers need to make several decisions that affect the quality and usefulness of\nlog data, e.g., where to place log statements and what information should be included in the\nlog message.",
    "section_title": "Introduction",
    "section_level": 2,
    "page_number": 1,
    "chunk_index": 6,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Introduction",
      "section_level": 2
    }
  },
  {
    "text": "In addition, log data can be voluminous and heterogeneous due to how\nindividual developers instrument an application and also the variety in a software stack\nthat compose a system. Those characteristics of log data make it exceedingly hard to\nmake optimal use of log data at scale. In addition, companies need to consider privacy,\nretention policies, and how to effectively get value from data. Even with the support of\nmachine learning and growing adoption of big data platforms, it is challenging to process\nand analyze data in a costly and timely manner.\nThe research community, including practitioners, have been actively working to address\nthe challenges related to the typical life-cycle of log, i.e., how to effectively provide\nbetter tooling support for logging decisions to developers (“Logging”), how to effectively\nprocess and store log data (“Logging Infrastructure”), and how to extract insights from log\ndata (“Log Analysis”). Previously, Rong et al. (2017) conducted a survey involving 41\nprimary studies to understand what was the current state of logging practices.",
    "section_title": "Introduction",
    "section_level": 2,
    "page_number": 1,
    "chunk_index": 7,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Introduction",
      "section_level": 2
    }
  },
  {
    "text": "They\nfocused their analysis on studies addressing development practices of logging. El-Masri\net al. (2020) conducted an in-depth analysis of 11 log parsing (referred as “log abstraction”)\ntechniques and proposed a quality model based on a survey of 89 primary studies on log\nanalysis and parsing. While these are useful, no overview exists that includes other\nimportant facets of log analysis (e.g., log analysis for quality assurance), connects the\ndifferent log-related areas, and identiﬁes the most pressing open challenges. This\nin-breadth knowledge is key not only to provide directions to the research community\nbut also to bridge the gap between the different research areas, and to summarize the\nliterature for easy access to practitioners.\nIn this paper, we propose a systematic mapping of the logging research area. To that\naim, we study 108 papers that appeared in top-level peer-reviewed conferences and\njournals from different communities (e.g., machine learning, software engineering, and\nsystems).",
    "section_title": "Introduction",
    "section_level": 2,
    "page_number": 1,
    "chunk_index": 8,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Introduction",
      "section_level": 2
    }
  },
  {
    "text": "We structure the research ﬁeld in light of the life-cycle of log data, elaborate the\nfocus of each research area, and discuss opportunities and directions for future work.\nOur analysis shows that (1) logging is a challenge not only in open-source projects but also\nin industry, (2) machine learning is a promising approach to enable contextual analysis of\nsource code for log recommendations but further investigation is required to assess the\nusability of those tools in practice, (3) few studies address efﬁcient persistence of log data,\nand (4) while log analysis is mature ﬁeld with several applications (e.g., quality assurance\nand failure prediction), there are open opportunities to analyze application logs and to\nevaluate state-of-the-art techniques in a DevOps context.\nCândido et al. (2021), PeerJ Comput. Sci., DOI 10.7717/peerj-cs.489\n3/38\n\n\nSURVEY METHODOLOGY\nThe goal of this paper is to discover, categorize, and summarize the key research results in\nlog-based software monitoring.",
    "section_title": "Introduction",
    "section_level": 2,
    "page_number": 1,
    "chunk_index": 9,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Introduction",
      "section_level": 2
    }
  },
  {
    "text": "To this end, we perform a systematic mapping study to\nprovide a holistic view of the literature in logging and automated log analysis. Concretely,\nwe investigate the following research questions:\n\u0001 RQ1: What are the publication trends in research on log-based monitoring over the years?\n\u0001 RQ2: What are the different research scopes of log-based monitoring?\nThe ﬁrst research question (RQ1) addresses the historical growth of the research ﬁeld.\nAnswering this research question enables us to identify the popular venues and the\ncommunities (e.g., Software Engineering, Distributed Systems) that have been focusing\non log-based monitoring innovation. Furthermore, we aim at investigating the participation\nof industry in the research ﬁeld. Researchers can beneﬁt from our analysis by helping them to\nmake a more informed decision regarding venues for paper submission. In addition, our\nanalysis also serves as a guide to practitioners willing to engage with the research community\neither by attending conferences or looking for references to study and experiment.",
    "section_title": "Introduction",
    "section_level": 2,
    "page_number": 1,
    "chunk_index": 10,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Introduction",
      "section_level": 2
    }
  },
  {
    "text": "The\nsecond research question (RQ2) addresses the actual mapping of the primary studies.\nAs illustrated in Fig. 1, the life-cycle of log data contains different inter-connected contexts\n(i.e., “Logging”, “Log Infrastructure”, and “Log Analysis”) with their own challenges and\nconcerns that span the entire development cycle. Answering this research question enables\nus to identify those concerns for each context and quantify the research effort by the number\nof primary studies in a particular category. In addition, we aim at providing an overview\nof the studies so practitioners and researchers are able to use our mapping study as a starting\npoint for an in-depth analysis of a particular topic of interest.\nOverall, we follow the standard guidelines for systematic mapping (Petersen et al., 2008).\nOur survey methodology is divided into four parts as illustrated in Fig. 2. First, we perform\npreliminary searches to derive our search criteria and build an initial list of potential\nrelevant studies based on ﬁve data sources.",
    "section_title": "Introduction",
    "section_level": 2,
    "page_number": 1,
    "chunk_index": 11,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Introduction",
      "section_level": 2
    }
  },
  {
    "text": "Next, we apply our inclusion/exclusion criteria\nto arrive at the eventual list of selected papers up to 2018 (when we ﬁrst conducted the\nsurvey). We then conduct the data extraction and classiﬁcation procedures. Finally, we\nupdate the results of our survey to include papers published in 2019.\nSearch Process\nStudy Selection\nsearch\nData\nsources\nQuery\nstrings\nearch Proce\nrefinement\nGoogle Scholar, \nACM, IEEE, \nScopus, Springer\nData\nurces\nInitial list\n(4,187)\nSelection\n(96)\nt\nS\nPreliminary list\n(315)\nstep 1\nselection criteria\nstep 2\nrank criteria\nClassification \nSchema\nsort\nn\nAbstracts\nPapers\nkeywording\nupdate\nClassification\nstep 1\nSurvey Update\ns\n'18 Papers\n(11)\n'19 Papers\n(12)\nfwd. snowballing\nFigure 2 Overview of survey methodology: our four steps consists of the discovery of related studies (“Search Process”), the selection of\nrelevant studies (“Study Selection”), the mapping process (“Classiﬁcation”), and the update for papers published in 2019 (“Survey\nUpdate”).\nFull-size\n\nDOI: 10.7717/peerj-cs.489/ﬁg-2\nCândido et al. (2021), PeerJ Comput.",
    "section_title": "Introduction",
    "section_level": 2,
    "page_number": 1,
    "chunk_index": 12,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Introduction",
      "section_level": 2
    }
  },
  {
    "text": "SURVEY METHODOLOGY\nThe goal of this paper is to discover, categorize, and summarize the key research results in\nlog-based software monitoring. To this end, we perform a systematic mapping study to\nprovide a holistic view of the literature in logging and automated log analysis. Concretely,\nwe investigate the following research questions:\n\u0001 RQ1: What are the publication trends in research on log-based monitoring over the years?\n\u0001 RQ2: What are the different research scopes of log-based monitoring?\nThe ﬁrst research question (RQ1) addresses the historical growth of the research ﬁeld.\nAnswering this research question enables us to identify the popular venues and the\ncommunities (e.g., Software Engineering, Distributed Systems) that have been focusing\non log-based monitoring innovation. Furthermore, we aim at investigating the participation\nof industry in the research ﬁeld. Researchers can beneﬁt from our analysis by helping them to\nmake a more informed decision regarding venues for paper submission.",
    "section_title": "Survey methodology",
    "section_level": 2,
    "page_number": 4,
    "chunk_index": 0,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Survey methodology",
      "section_level": 2
    }
  },
  {
    "text": "In addition, our\nanalysis also serves as a guide to practitioners willing to engage with the research community\neither by attending conferences or looking for references to study and experiment. The\nsecond research question (RQ2) addresses the actual mapping of the primary studies.\nAs illustrated in Fig. 1, the life-cycle of log data contains different inter-connected contexts\n(i.e., “Logging”, “Log Infrastructure”, and “Log Analysis”) with their own challenges and\nconcerns that span the entire development cycle. Answering this research question enables\nus to identify those concerns for each context and quantify the research effort by the number\nof primary studies in a particular category. In addition, we aim at providing an overview\nof the studies so practitioners and researchers are able to use our mapping study as a starting\npoint for an in-depth analysis of a particular topic of interest.\nOverall, we follow the standard guidelines for systematic mapping (Petersen et al., 2008).\nOur survey methodology is divided into four parts as illustrated in Fig. 2.",
    "section_title": "Survey methodology",
    "section_level": 2,
    "page_number": 4,
    "chunk_index": 1,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Survey methodology",
      "section_level": 2
    }
  },
  {
    "text": "First, we perform\npreliminary searches to derive our search criteria and build an initial list of potential\nrelevant studies based on ﬁve data sources. Next, we apply our inclusion/exclusion criteria\nto arrive at the eventual list of selected papers up to 2018 (when we ﬁrst conducted the\nsurvey). We then conduct the data extraction and classiﬁcation procedures. Finally, we\nupdate the results of our survey to include papers published in 2019.\nSearch Process\nStudy Selection\nsearch\nData\nsources\nQuery\nstrings\nearch Proce\nrefinement\nGoogle Scholar, \nACM, IEEE, \nScopus, Springer\nData\nurces\nInitial list\n(4,187)\nSelection\n(96)\nt\nS\nPreliminary list\n(315)\nstep 1\nselection criteria\nstep 2\nrank criteria\nClassification \nSchema\nsort\nn\nAbstracts\nPapers\nkeywording\nupdate\nClassification\nstep 1\nSurvey Update\ns\n'18 Papers\n(11)\n'19 Papers\n(12)\nfwd. snowballing\nFigure 2 Overview of survey methodology: our four steps consists of the discovery of related studies (“Search Process”), the selection of\nrelevant",
    "section_title": "Survey methodology",
    "section_level": 2,
    "page_number": 4,
    "chunk_index": 2,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Survey methodology",
      "section_level": 2
    }
  },
  {
    "text": "studies (“Study Selection”), the mapping process (“Classiﬁcation”), and the update for papers published in 2019 (“Survey\nUpdate”).\nFull-size\n\nDOI: 10.7717/peerj-cs.489/ﬁg-2\nCândido et al. (2021), PeerJ Comput. Sci., DOI 10.7717/peerj-cs.489\n4/38\n\n\nData sources and search process\nTo conduct our study, we considered ﬁve popular digital libraries from different publishers\nbased on other literature reviews in software engineering, namely, ACM Digital Library,\nIEEE Xplore, SpringerLink, Scopus, and Google Scholar. By considering ﬁve digital\nlibraries, we maximize the range of venues and increase the diversity of studies related\nto logging. In addition, this decision reduces the bias caused by the underlying search\nengine since two digital libraries may rank the results in a different way for the same\nequivalent search.\nWe aim to discover relevant papers from different areas as much as possible.",
    "section_title": "Survey methodology",
    "section_level": 2,
    "page_number": 4,
    "chunk_index": 3,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Survey methodology",
      "section_level": 2
    }
  },
  {
    "text": "However,\nit is a challenge to build an effective query for the ﬁve selected digital libraries without\ndealing with a massive amount of unrelated results, since terms such as “log” and “log\nanalysis” are pervasive in many areas. Conversely, inﬂating the search query with\nspeciﬁc terms to reduce false positives would bias our study to a speciﬁc context (e.g., log\nanalysis for debugging). To ﬁnd a balance between those cases, we conducted preliminary\nsearches with different terms and search scopes, e.g., full text, title, and abstract. We\nconsidered terms based on “log”, its synonyms, and activities related to log analysis.\nDuring this process, we observed that forcing the presence of the term “log” helps to order\nrelevant studies on the ﬁrst pages. In case the data source is unable to handle word\nstemming automatically (e.g., “log” and “logging”), we enhance the query with the\nkeywords variations. In addition, conﬁgured the data sources to search on titles and\nabstracts whenever it was possible.",
    "section_title": "Survey methodology",
    "section_level": 2,
    "page_number": 4,
    "chunk_index": 4,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Survey methodology",
      "section_level": 2
    }
  },
  {
    "text": "In case the data source provides no support to search on\ntitles and abstracts, we considered only titles to reduce false positives. This process resulted\nin the following search query:\nlog AND (trace OR event OR software OR system OR code OR detect OR mining OR\nanalysis OR monitoring OR web OR technique OR develop OR pattern OR practice)\nDealing with multiple libraries requires additional work to merge data and remove\nduplicates. In some cases, the underlying information retrieval algorithms yielded\nunexpected results when querying some libraries, such as duplicates within the data source\nand entries that mismatch the search constraints. To overcome those barriers, we\nimplemented auxiliary scripts to cleanup the dataset. We index the entries by title to\neliminate duplicates, and we remove entries that fail to match the search criteria.\nFurthermore, we keep the most recent work when we identify two entries with the same\ntitle and different publication date (e.g., journal extension from previous work).",
    "section_title": "Survey methodology",
    "section_level": 2,
    "page_number": 4,
    "chunk_index": 5,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Survey methodology",
      "section_level": 2
    }
  },
  {
    "text": "As of December of 2018, when we ﬁrst conducted this search, we extracted 992 entries\nfrom Google Scholar, 1,122 entries from ACM Digital Library, 1,900 entries from IEEE\nXplore, 2,588 entries from Scopus, and 7,895 entries from SpringerLink (total of 14,497\nentries). After merging and cleaning the data, we ended up with 4,187 papers in our initial\nlist.\nStudy selection\nWe conduct the selection process by assessing the 4,187 entries according to inclusion/\nexclusion criteria and by selecting publications from highly ranked venues. We deﬁne the\ncriteria as follows:\nCândido et al. (2021), PeerJ Comput. Sci., DOI 10.7717/peerj-cs.489\n5/38\n\n\n\u0001 C1: It is an English manuscript.\n\u0001 C2: It is a primary study.\n\u0001 C3: It is a full research paper accepted through peer-review.\n\u0001 C4: The paper uses the term “log” in a software engineering context, i.e., logs to describe\nthe behavior of a software system. We exclude papers that use the term “log” in an\nunrelated semantic (e.g., deforestation, life logging, well logging, log function).",
    "section_title": "Survey methodology",
    "section_level": 2,
    "page_number": 4,
    "chunk_index": 6,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Survey methodology",
      "section_level": 2
    }
  },
  {
    "text": "The rationale for criterion C1 is that major venues use English as the standard idiom for\nsubmission. The rationale for criterion C2 is to avoid including secondary studies in our\nmapping, as suggested by Kitchenham & Charters (2007). In addition, the process of\napplying this criterion allows us to identify other systematic mappings and systematic\nliterature reviews related to ours. The rationale for criterion C3 is that some databases\nreturn gray literature as well as short papers; our focus is on full peer-reviewed research\npapers, which we consider mature research, ready for real-world tests. Note that different\nvenues might have different page number speciﬁcations to determine whether a\nsubmission is a full or short paper, and these speciﬁcations might change over time.\nWe consulted the page number from each venue to avoid unfair exclusion. The rationale\nfor criterion C4 is to exclude papers that are unrelated to the scope of this mapping\nstudy. We noticed that some of the results are in the context of, e.g., mathematics and\nenvironmental studies.",
    "section_title": "Survey methodology",
    "section_level": 2,
    "page_number": 4,
    "chunk_index": 7,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Survey methodology",
      "section_level": 2
    }
  },
  {
    "text": "While we could have tweaked our search criteria to minimize\nthe occurrence of those false positives (e.g., NOT deforestation), we were unable to\nsystematically derive all keywords to exclude; therefore, we favored higher false positive\nrate in exchange of increasing the chances of discovering relevant papers.\nThe ﬁrst author manually performed the inclusion procedure. He analyzed the title and\nabstracts of all the papers marking the paper as “in” or “out”. During this process, the\nauthor applied the criteria and categorized the reasons for exclusion. For instance,\nwhenever an entry fails the criteria C4, the authors classiﬁed it as “Out of Scope”. The\ncategories we used are: “Out of Scope”, “Short/workshop paper”, “Not a research paper”,\n“Unpublished” (e.g., unpublished self-archived paper indexed by Google Scholar),\n“Secondary study”, and “Non-English manuscript”. It is worth mentioning that we ﬂagged\nthree entries as “Duplicate” as our merging step missed these cases due to special\ncharacters in the title. After applying the selection criteria, we removed 3,872 entries\nresulting in 315 entries.",
    "section_title": "Survey methodology",
    "section_level": 2,
    "page_number": 4,
    "chunk_index": 8,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Survey methodology",
      "section_level": 2
    }
  },
  {
    "text": "In order to ﬁlter the remaining 315 papers by rank, we used the CORE Conference\nRank (CORE Rank) (http://www.core.edu.au/conference-portal) as a reference. We\nconsidered studies published only in venues ranked as A* or A. According to the CORE\nRank, those categories indicate that the venue is widely known in the computer science\ncommunity and has a strict review process by experienced researches. After applying the\nrank criteria, we removed 219 papers.\nOur selection consists of (315 −219 =) 96 papers after applying inclusion/exclusion\ncriteria (step 1) and ﬁltering by venue rank (step 2). Table 1 summarises the selection\nprocess.\nCândido et al. (2021), PeerJ Comput. Sci., DOI 10.7717/peerj-cs.489\n6/38\n\n\nData extraction and classification\nWe focus the data extraction process to the required data to answer our research\nquestions.\nTo answer RQ1, we collect metadata from the papers and their related venues.\nConcretely, we deﬁne the following schema: “Year of publication”, “Type of publication”,\n“Venue name”, and “Research community”.",
    "section_title": "Survey methodology",
    "section_level": 2,
    "page_number": 4,
    "chunk_index": 9,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Survey methodology",
      "section_level": 2
    }
  },
  {
    "text": "The ﬁelds “Year of publication” and\n“Venue name” are readly available on the scrapped data from the data sources. To extract\nthe ﬁeld “Type of publication”, we automatically assign the label “journal” if it is a\njournal paper. For conference papers, we manually check the proceedings to determine\nif it is a “research track” or “industry track” paper (we assume “research track” if not\nexplicitly stated). To extract the ﬁeld “Research community”, we check the topics of\ninterest from the conferences and journals. This information is usually available in a “call\nfor papers” page. Later, we manually aggregate the venues and we merge closely related\ntopics (e.g., Artiﬁcial Intelligence, Machine Learning, and Data Science). While a\nTable 1 Distribution of study selection when the survey was ﬁrst conducted.\nSelection Step\nQty\nStep 1. Exclusion by selection criteria\n3,872\nOut of scope (failed C4)\n3,544\nShort/workshop paper (failed C3)\n276\nNot a research paper (failed C3)\n40\nNon-English manuscri",
    "section_title": "Survey methodology",
    "section_level": 2,
    "page_number": 4,
    "chunk_index": 10,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Survey methodology",
      "section_level": 2
    }
  },
  {
    "text": "pt (failed C1)\n4\nUnpublished (failed C3)\n3\nDuplicate\n3\nSecondary study (failed C2)\n2\nPreliminary inclusion of papers\n315\nStep 2. Exclusion by venue rank (neither A* nor A)\n219\nUnranked\n143\nRank B\n47\nRank C\n30\nInclusion of papers (up to 2018, inclusive)\n96\nextract\nTopics of \nInterest\nConference\nProceedings\nce\ngs\nCall for\nPapers\nResearch community\nextract\nretrieve\nextract\nPrimary\nStudy\nYear of publication\nType of publication\nResearch track, Industry track, Journal\nVenue name\nextract\nrefinement\nFigure 3 Data extraction for RQ1.\nFull-size\n\nDOI: 10.7717/peerj-cs.489/ﬁg-3\nCândido et al. (2021), PeerJ Comput. Sci., DOI 10.7717/peerj-cs.489\n7/38\n\n\ncomplete meta-analysis is out of scope from our study, we believe the extracted data is\nsufﬁcient to address the research question. Figure 3 summarizes the process for RQ1.\nTo answer RQ2, we collect the abstracts from the primary studies. In this process, we\nstructure the abstract to better identify the motivation of the study, what problem the\nauthors are addressing, how the researchers are mitigating the problem, and the results of\nthe study.",
    "section_title": "Survey methodology",
    "section_level": 2,
    "page_number": 4,
    "chunk_index": 11,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Survey methodology",
      "section_level": 2
    }
  },
  {
    "text": "Given the diverse set of problems and domains, we ﬁrst group the studies\naccording to their overall context (e.g., whether the paper relates to “Logging”, “Log\nInfrastructure”, or “Log Analysis”). To mitigate self-bias, we conducted two independent\ntriages and compared our results. In case of divergence, we review the paper in depth to\nassign the context that better ﬁts the paper. To derive the classiﬁcation schemafor each\ncontext, we perform the keywording of abstracts (Petersen et al., 2008). In this process,\nwe extract keywords in the abstract (or introduction, if necessary) and cluster similar\nkeywords to create categories. We perform this process using a random sample of papers\nto derive an initial classiﬁcation schema.\nLater, with all the papers initially classiﬁed, the authors explored the speciﬁc objectives\nof each paper and review the assigned category. To that aim, the ﬁrst and second authors\nperformed card sorting (Spencer & Warfel, 2004; Usability.gov, 2019) to determine the\ngoal of each of the studied papers.",
    "section_title": "Survey methodology",
    "section_level": 2,
    "page_number": 4,
    "chunk_index": 12,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Survey methodology",
      "section_level": 2
    }
  },
  {
    "text": "Note that, in case new categories emerge in this process,\nwe generalize them in either one of the existing categories or enhance our classiﬁcation\nschema to update our view of different objectives in a particular research area. After the\nﬁrst round of card sorting, we noticed that some of the groups (often the ones with high\nnumber of papers) could be further broken down in subcategories (we discuss the\ncategories and related subcategories in the Results section).\nThe ﬁrst author conducted two separate blinded classiﬁcations on different periods of\ntime to measure the degree of adherence to the schema given that classiﬁcation is subject\nof interpretation, and thus, a source of bias. The same outcome converged on 83% of\nthe cases (80 out of the 96 identiﬁed papers). The divergences were then discussed with the\nsecond author of this paper. Furthermore, the second author reviewed the resulting\nclassiﬁcation.",
    "section_title": "Survey methodology",
    "section_level": 2,
    "page_number": 4,
    "chunk_index": 13,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Survey methodology",
      "section_level": 2
    }
  },
  {
    "text": "Note that, while a paper may address more than one category, we choose the\nLogging, Log Infrastructure, Log Analysis\nclustering\nClassification schema\nAbstract\nextract\nStructured\nAbstract\nPaper context\ntriage\nextract\nKeywords\nupdate\nreview\nPrimary\nStudies\nFigure 4 Data extraction and classiﬁcation for RQ2. The dashed arrows denote the use of the data\nschema by the researchers with the primary studies.\nFull-size\n\nDOI: 10.7717/peerj-cs.489/ﬁg-4\nCândido et al. (2021), PeerJ Comput. Sci., DOI 10.7717/peerj-cs.489\n8/38\n\n\ncategory related to the most signiﬁcant contribution of that paper. Figure 4 summarizes the\nprocess for RQ2.\nSurvey update\nAs of October of 2020, we updated our survey to include papers published in 2019 since\nwe ﬁrst conducted this analysis during December in 2018. To this end, we select all 11\npapers from 2018 and perform forward snowballing to fetch a preliminary list of papers\nfrom 2019. We use snowballing for simplicity since we can leverage the “Cited By” feature\nfrom Google Scholar rather than scraping data of all ﬁve digital libraries.",
    "section_title": "Survey methodology",
    "section_level": 2,
    "page_number": 4,
    "chunk_index": 14,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Survey methodology",
      "section_level": 2
    }
  },
  {
    "text": "It is worth\nmentioning that we limit the results up to 2019 to avoid incomplete results for 2020.\nFor the preliminary list of 2019, we apply the same selection and rank criteria\n(see Section “Study Selection”); then, we analyze and map the studies according to the\nexisting classiﬁcation schema (see Section “Data Extraction and Classiﬁcation”). In this\nprocess, we identify 12 new papers and merge them with our existing dataset. Our ﬁnal\ndataset consists of (96 + 12 =) 108 papers.\nRESULTS\nPublication trends (RQ1)\nFigure 5 highlights the growth of publication from 1992 to 2019. The interest on logging\nhas been continuously increasing since the early 2000’s. During this time span, we\nobserved the appearance of industry track papers reporting applied research in a real\ncontext. This gives some evidence that the growing interest on the topic attracted not only\nresearchers from different areas but also companies, fostering the collaboration between\nacademia and industry.",
    "section_title": "Survey methodology",
    "section_level": 2,
    "page_number": 4,
    "chunk_index": 15,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Survey methodology",
      "section_level": 2
    }
  },
  {
    "text": "We identiﬁed 108 papers (72 research track papers, 24 journals, and 12 industry track\npapers) published in 46 highly ranked venues spanning different communities (Table 2).\nTable 2 highlights the distribution of venues grouped by the research community,\ne.g., there are 44 papers published on 10 Software Engineering venues.\n1\n1\n1\n2\n1\n2\n1\n1\n1\n3\n1\n1\n5\n1\n6\n3\n1\n5\n2\n4\n2\n8\n1\n4\n2\n2\n6\n1\n3\n6\n5\n2\n5\n5\n1\n8\n3\n1\n'92\n'98\n'07\n'03 '04 '05 '06\n'08\n'10\n'09\n'12\n'11\n'13\n'15 '16\n'14\n'18\n'00\n'17\n'19\nYear\nPublication Type\nindustry track paper\njournal\nresearch track paper\nFigure 5 Growth of publication types over the years. Labels indicate the number of publication per\ntype in a speciﬁc year. There are 108 papers in total.\nFull-size\n\nDOI: 10.7717/peerj-cs.489/ﬁg-5\nCândido et al. (2021), PeerJ Comput. Sci., DOI 10.7717/peerj-cs.489\n9/38",
    "section_title": "Survey methodology",
    "section_level": 2,
    "page_number": 4,
    "chunk_index": 16,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Survey methodology",
      "section_level": 2
    }
  },
  {
    "text": "category related to the most signiﬁcant contribution of that paper. Figure 4 summarizes the\nprocess for RQ2.\nSurvey update\nAs of October of 2020, we updated our survey to include papers published in 2019 since\nwe ﬁrst conducted this analysis during December in 2018. To this end, we select all 11\npapers from 2018 and perform forward snowballing to fetch a preliminary list of papers\nfrom 2019. We use snowballing for simplicity since we can leverage the “Cited By” feature\nfrom Google Scholar rather than scraping data of all ﬁve digital libraries. It is worth\nmentioning that we limit the results up to 2019 to avoid incomplete results for 2020.\nFor the preliminary list of 2019, we apply the same selection and rank criteria\n(see Section “Study Selection”); then, we analyze and map the studies according to the\nexisting classiﬁcation schema (see Section “Data Extraction and Classiﬁcation”). In this\nprocess, we identify 12 new papers and merge them with our existing dataset. Our ﬁnal\ndataset consists of (96 + 12 =) 108 papers.",
    "section_title": "Results",
    "section_level": 2,
    "page_number": 9,
    "chunk_index": 0,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Results",
      "section_level": 2
    }
  },
  {
    "text": "RESULTS\nPublication trends (RQ1)\nFigure 5 highlights the growth of publication from 1992 to 2019. The interest on logging\nhas been continuously increasing since the early 2000’s. During this time span, we\nobserved the appearance of industry track papers reporting applied research in a real\ncontext. This gives some evidence that the growing interest on the topic attracted not only\nresearchers from different areas but also companies, fostering the collaboration between\nacademia and industry.\nWe identiﬁed 108 papers (72 research track papers, 24 journals, and 12 industry track\npapers) published in 46 highly ranked venues spanning different communities (Table 2).\nTable 2 highlights the distribution of venues grouped by the research community,\ne.g., there are 44 papers published on 10 Software Engineering venues.\n1\n1\n1\n2\n1\n2\n1\n1\n1\n3\n1\n1\n5\n1\n6\n3\n1\n5\n2\n4\n2\n8\n1\n4\n2\n2\n6\n1\n3\n6\n5\n2\n5\n5\n1\n8\n3\n1\n'92\n'98\n'07\n'03 '04 '05 '06\n'08\n'10\n'09\n'12\n'11\n'13\n'15 '16\n'14\n'18\n'00\n'17\n'19\nYear\nPublication Type\nindustry track paper\njournal\nresearch track paper\nFigure 5 Growth of publication types over the years.",
    "section_title": "Results",
    "section_level": 2,
    "page_number": 9,
    "chunk_index": 1,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Results",
      "section_level": 2
    }
  },
  {
    "text": "Labels indicate the number of publication per\ntype in a speciﬁc year. There are 108 papers in total.\nFull-size\n\nDOI: 10.7717/peerj-cs.489/ﬁg-5\nCândido et al. (2021), PeerJ Comput. Sci., DOI 10.7717/peerj-cs.489\n9/38\n\n\nTable 3 highlights the most recurring venues in our dataset (we omitted venues with less\nthan three papers for brevity). The “International Conference on Software Engineering\n(ICSE)”, the “Empirical Software Engineering Journal (EMSE)”, and the “International\nConference on Dependable Systems and Networks (DSN)” are the top three recurring\nvenues related to the subject and are well-established venues. DSN and ICSE are\nconferences with more than 40 editions each and EMSE is a journal with an average of\nﬁve issues per year since 1996. At a glance, we noticed that papers from DSN have an\nemphasis on log analysis of system logs while papers from ICSE and EMSE have an\nemphasis on development aspects of logging practices (more details about the research\nareas in the next section). Note that Table 3 also concentrates 65% (71 out of 108) of the\nprimary studies in our dataset.",
    "section_title": "Results",
    "section_level": 2,
    "page_number": 9,
    "chunk_index": 2,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Results",
      "section_level": 2
    }
  },
  {
    "text": "Overview of research areas (RQ2)\nWe grouped the studied papers among the following three categories based in our\nunderstanding about the life-cycle of log data (see Fig. 1). For each category, we derived\nsubcatories that emerged from our keywording process (see Section “Data Extraction and\nClassiﬁcation”):\n\u0001 LOGGING: Research in this category aims at understanding how developers conduct\nlogging practices and providing better tooling support to developers. There are\nthree subcategories in this line of work: (1) empirical studies on logging practices,\n(2) requirements for application logs, and (3) implementation of log statements\n(e.g., where and how to log).\n\u0001 LOG INFRASTRUCTURE: Research in this category aims at improving log processing and\npersistence. There are two subcategories in this line of work: (1) log parsing, and (2) log\nstorage.\n\u0001 LOG ANALYSIS: Research in this category aims at extracting knowledge from log data.",
    "section_title": "Results",
    "section_level": 2,
    "page_number": 9,
    "chunk_index": 3,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Results",
      "section_level": 2
    }
  },
  {
    "text": "There are eight subcategories in this line of work: (1) anomaly detection, (2) security and\nprivacy, (3) root cause analysis, (4) failure prediction, (5) quality assurance, (6) model\ninference and invariant mining, (7) reliability and dependability, and (8) log platforms.\nTable 2 Distribution of venues and publications grouped by research communities.\nResearch community\n# of venues\n# of papers\nSoftware Engineering\n10\n44\nDistributed Systems and Cloud Computing\n10\n20\nSystems\n9\n17\nArtiﬁcial Intelligence, Machine Learning, and Data Science (AI)\n8\n13\nSecurity\n5\n7\nInformation Systems\n3\n6\nDatabases\n1\n1\nTotal\n46\n108\nCândido et al. (2021), PeerJ Comput. Sci., DOI 10.7717/peerj-cs.489\n10/38\n\n\nWe provide an overview of the categories, their respective descriptions, and summary of\nour results in Table 4. In summary, we observed that LOG ANALYSIS dominates most of the\nresearch effort (68 out of 108 papers) with papers published since the early 90’s. LOG\nINFRASTRUCTURE is younger than LOG ANALYSIS as we observed papers starting from 2007\n(16 out of 108 papers).",
    "section_title": "Results",
    "section_level": 2,
    "page_number": 9,
    "chunk_index": 4,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Results",
      "section_level": 2
    }
  },
  {
    "text": "LOGGING is the youngest area of interest with an increasing\nmomentum for research (24 out of 108 papers). In the following, we elaborate our analysis\nand provide an overview of the primary studies.\nLogging\nLog messages are usually in the form of free text and may expose parts of the system\nstate (e.g., exceptions and variable values) to provide additional context. The full log\nstatement also includes a severity level to indicate the purpose of that statement. Logging\nTable 3 Top recurring venues ordered by number of papers. There are 14 (out of 46) recurring venues with at least three papers published\n(omitted venues with less than three papers for brevity).\nVenue (acronym)\nReferences\nQty\nInternational Conference on Software Engineering\n(ICSE)\nAndrews & Zhang (2003), Yuan, Park & Zhou (2012), Beschastnikh et al. (2014), Fu et al.\n(2014a), Pecchia et al. (2015), Zhu et al. (2015), Lin et al. (2016), Chen & Jiang (2017a), Li\net al. (2019b), Zhu et al.",
    "section_title": "Results",
    "section_level": 2,
    "page_number": 9,
    "chunk_index": 5,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Results",
      "section_level": 2
    }
  },
  {
    "text": "(2019)\n10\nEmpirical Software Engineering Journal (EMSE)\nHuynh & Miller (2009), Shang, Nagappan & Hassan (2015), Russo, Succi & Pedrycz (2015),\nChen & Jiang (2017b), Li, Shang & Hassan (2017), Hassani et al. (2018), Li et al. (2018),\nZeng et al. (2019), Li et al. (2019a)\n9\nIEEE/IFIP International Conference on\nDependable Systems and Networks (DSN)\nOliner & Stearley (2007), Lim, Singh & Yajnik (2008), Cinque et al. (2010), Di Martino,\nCinque & Cotroneo (2012), El-Sayed & Schroeder (2013), Oprea et al. (2015), He et al.\n(2016a), Neves, Machado & Pereira (2018)\n8\nInternational Symposium on Software Reliability\nEngineering (ISSRE)\nTang & Iyer (1992), Mariani & Pastore (2008), Banerjee, Srikanth & Cukic (2010), Pecchia &\nRusso (2012), Farshchi et al. (2015), He et al. (2016b), Bertero et al. (2017)\n7\nInternational Conference on Automated Software\nEngineering (ASE)\nAndrews (1998), Chen et al. (2018), He et al. (2018a), Ren et al. (2019), Liu et al. (2019a)\n5\nInternational Symposium on Reliable Distributed\nSystems (SRDS)\nZhou et al. (2010), Kc & Gu (2011), Fu et al. (2012), Chuah et al.",
    "section_title": "Results",
    "section_level": 2,
    "page_number": 9,
    "chunk_index": 6,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Results",
      "section_level": 2
    }
  },
  {
    "text": "(2013), Gurumdimma et al.\n(2016)\n5\nACM International Conference on Knowledge\nDiscovery and Data Mining (KDD)\nMakanju, Zincir-Heywood & Milios (2009), Nandi et al. (2016), Wu, Anchuri & Li (2017), Li\net al. (2017)\n4\nIEEE International Symposium on Cluster, Cloud\nand Grid Computing (CCGrid)\nPrewett (2005), Yoon & Squicciarini (2014), Lin et al. (2015), Di et al. (2017)\n4\nIEEE Transactions on Software Engineering (TSE) Andrews & Zhang (2003), Tian, Rudraraju & Li (2004), Cinque, Cotroneo & Pecchia (2013),\nLiu et al. (2019b)\n4\nAnnual Computer Security Applications\nConference (ACSAC)\nAbad et al. (2003), Barse & Jonsson (2004), Yen et al. (2013)\n3\nIBM Journal of Research and Development\nAharoni et al. (2011), Ramakrishna et al. (2017), Wang et al. (2017)\n3\nInternational Conference on Software\nMaintenance and Evolution (ICSME)\nShang et al. (2014), Zhi et al. (2019), Anu et al. (2019)\n3\nIEEE International Conference on Data Mining\n(ICDM)\nFu et al. (2009), Xu et al. (2009a), Tang & Li (2010)\n3\nJournal of Systems and Software (JSS)\nMavridis & Karatza (2017), Bao et al. (2018), Farshchi et al.",
    "section_title": "Results",
    "section_level": 2,
    "page_number": 9,
    "chunk_index": 7,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Results",
      "section_level": 2
    }
  },
  {
    "text": "(2018)\n3\nTotal\n71\nCândido et al. (2021), PeerJ Comput. Sci., DOI 10.7717/peerj-cs.489\n11/38\n\n\nframeworks provide developers with different log levels: debug for low level logging, info to\nprovide information on the system execution, error to indicate unexpected state that\nmay compromise the normal execution of the application, and fatal to indicate a severe\nstate that might terminate the execution of the application. Logging an application involves\nTable 4 Summary of our mapping study. The 108 papers are grouped into three main research areas, and each area has subcategories according to\nthe focus of the study.\nCategory\nDescription\nPapers\nQty\nLogging: The development of effective logging code\n24\nEmpirical Studies\nUnderstanding and insights about how\ndevelopers conduct logging in general\nYuan, Park & Zhou (2012), Chen & Jiang (2017b), Shang et al. (2014), Shang,\nNagappan & Hassan (2015), Pecchia et al. (2015), Kabinna et al. (2016), Li\net al. (2019b), Zeng et al. (2019)\n8\nLog requirements\nAssessment of log conformance given a\nknown requirement\nCinque et al.",
    "section_title": "Results",
    "section_level": 2,
    "page_number": 9,
    "chunk_index": 8,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Results",
      "section_level": 2
    }
  },
  {
    "text": "(2010), Pecchia & Russo (2012), Cinque, Cotroneo & Pecchia\n(2013), Yuan et al. (2012), da Cruz et al. (2004)\n5\nImplementation of\nlog statements\nFocus on what to log, where to log, and\nhow to log\nChen & Jiang (2017a), Hassani et al. (2018), Fu et al. (2014a), Zhu et al. (2015),\nLi et al. (2018), Li, Shang & Hassan (2017), He et al. (2018a), Li et al. (2019a),\nLiu et al. (2019b), Anu et al. (2019), Zhi et al. (2019)\n11\nLog Infrastructure: Techniques to enable and fulﬁl the requirements of the analysis process\n16\nParsing\nExtraction of log templates from raw log\ndata\nAharon et al. (2009), Makanju, Zincir-Heywood & Milios (2009), Makanju,\nZincir-Heywood & Milios (2012), Liang et al. (2007), Gainaru et al. (2011),\nHamooni et al. (2016), Zhou et al. (2010), Lin et al. (2016), Tang & Li (2010),\nHe et al. (2016a), He et al. (2018b), Zhu et al. (2019), Agrawal, Karlupia &\nGupta (2019)\n13\nStorage\nEfﬁcient persistence of large datasets of\nlogs\nLin et al. (2015), Mavridis & Karatza (2017), Liu et al.",
    "section_title": "Results",
    "section_level": 2,
    "page_number": 9,
    "chunk_index": 9,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Results",
      "section_level": 2
    }
  },
  {
    "text": "(2019a)\n3\nLog Analysis: Insights from processed log data\n68\nAnomaly detection\nDetection of abnormal behaviour\nTang & Iyer (1992), Oliner & Stearley (2007), Lim, Singh & Yajnik (2008), Xu\net al. (2009b), Xu et al. (2009a), Fu et al. (2009), Ghanbari, Hashemi & Amza\n(2014), Gao et al. (2014), Juvonen, Sipola & Hämäläinen (2015), Farshchi et al.\n(2015), He et al. (2016b), Nandi et al. (2016), Du et al. (2017), Bertero et al.\n(2017), Lu et al. (2017), Debnath et al. (2018), Bao et al. (2018), Farshchi et al.\n(2018), Zhang et al. (2019), Meng et al. (2019)\n20\nSecurity and\nprivacy\nIntrusion and attack detection\nOprea et al. (2015), Chu et al. (2012), Yoon & Squicciarini (2014), Yen et al.\n(2013), Barse & Jonsson (2004), Abad et al. (2003), Prewett (2005), Butin & Le\nMétayer (2014), Goncalves, Bota & Correia (2015)\n9\nRoot cause analysis\nAccurate failure identiﬁcation and\nimpact analysis\nGurumdimma et al. (2016), Kimura et al. (2014), Pi et al. (2018), Chuah et al.\n(2013), Zheng et al. (2011), Ren et al.",
    "section_title": "Results",
    "section_level": 2,
    "page_number": 9,
    "chunk_index": 10,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Results",
      "section_level": 2
    }
  },
  {
    "text": "(2019)\n6\nFailure prediction\nAnticipating failures that leads a system\nto an unrecoverable state\nWang et al. (2017), Fu et al. (2014b), Russo, Succi & Pedrycz (2015), Khatuya\net al. (2018), Shalan & Zulkernine (2013), Fu et al. (2012)\n6\nQuality assurance\nLogs as support for quality assurance\nactivities\nAndrews (1998), Andrews & Zhang (2000), Andrews & Zhang (2003), Chen et al.\n(2018)\n4\nModel inference\nand invariant\nmining\nModel and invariant checking\nUlrich et al. (2003), Mariani & Pastore (2008), Tan et al. (2010), Beschastnikh\net al. (2014), Wu, Anchuri & Li (2017), Awad & Menasce (2016), Kc & Gu\n(2011), Lou et al. (2010), Steinle et al. (2006), Di Martino, Cinque & Cotroneo\n(2012)\n10\nReliability and\ndependability\nUnderstand dependability properties of\nsystems (e.g., reliability, performance)\nBanerjee, Srikanth & Cukic (2010), Tian, Rudraraju & Li (2004), Huynh &\nMiller (2009), El-Sayed & Schroeder (2013), Ramakrishna et al. (2017), Park\net al. (2017)\n6\nLog platforms\nFull-ﬂedged log analysis platforms\nLi et al. (2017), Aharoni et al. (2011), Yu et al. (2016), Balliu et al. (2015), Di et al.",
    "section_title": "Results",
    "section_level": 2,
    "page_number": 9,
    "chunk_index": 11,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Results",
      "section_level": 2
    }
  },
  {
    "text": "(2017), Neves, Machado & Pereira (2018), Gunter et al. (2007)\n7\nCândido et al. (2021), PeerJ Comput. Sci., DOI 10.7717/peerj-cs.489\n12/38\n\n\nseveral decisions such as what to log. These are all important decisions since they have a\ndirect impact on the effectiveness of the future analysis. Excessive logging may cause\nperformance degradation due the number of writing operations and might be costly in\nterms of storage. Conversely, insufﬁcient information undermines the usefulness of the\ndata to the operations team. It is worth mentioning that the underlying environment\nalso provides valuable data. Environment logs provide insights about resource usage\n(e.g., CPU, memory and network) and this data can be correlated with application logs\non the analysis process. In contrast to application logs, developers are often not in control\nof environment logs. On the other hand, they are often highly structured and are useful as a\ncomplementary data source that provides additional context.\nLOGGING deals with the decisions from the developer’s perspective.",
    "section_title": "Results",
    "section_level": 2,
    "page_number": 9,
    "chunk_index": 12,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Results",
      "section_level": 2
    }
  },
  {
    "text": "Developers have to\ndecide the placement of log statements, what message description to use, which runtime\ninformation is relevant to log (e.g., the thrown exception), and the appropriate severity\nlevel. Efﬁcient and accurate log analysis rely on the quality of the log data, but it is not\nalways feasible to know upfront the requirements of log data during development time.\nWe observed three different subcategories in log engineering: (1) empirical studies on\nlog engineering practices, (2) techniques to improve log statements based on known\nrequirements for log data, and (3) techniques to help developers make informed decisions\nwhen implementing log statements (e.g., where and how to log). In the following,\nwe discuss the 24 log engineering papers in the light of these three types of studies.\nEmpirical studies\nUnderstanding how practitioners deal with the log engineering process in a real scenario is\nkey to identify open problems and provide research directions. Papers in this category aim\nat addressing this agenda through empirical studies in open-source projects (and their\ncommunities).",
    "section_title": "Results",
    "section_level": 2,
    "page_number": 9,
    "chunk_index": 13,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Results",
      "section_level": 2
    }
  },
  {
    "text": "Yuan, Park & Zhou (2012) conducted the ﬁrst empirical study focused on\nunderstanding logging practices. They investigated the pervasiveness of logging, the\nbeneﬁts of logging, and how log-related code changes over time in four open-source\nprojects (Apache httpd, OpenSSH, PostgresSQL, and Squid). In summary, while logging\nwas widely adopted in the projects and were beneﬁcial for failure diagnosis, they show that\nlogging as a practice relies on the developer’s experience. Most of the recurring changes\nwere updates to the content of the log statement.\nLater, Chen & Jiang (2017b) conducted a replication study with a broader corpus: 21\nJava-based projects from the Apache Foundation. Both studies conﬁrm that logging code is\nactively maintained and that log changes are recurrent; however, the presence of log data in\nbug reports are not necessarily correlated to the resolution time of bug ﬁxes (Chen & Jiang,\n2017b). This is understandable as resolution time also relates to the complexity of the\nreported issue.",
    "section_title": "Results",
    "section_level": 2,
    "page_number": 9,
    "chunk_index": 14,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Results",
      "section_level": 2
    }
  },
  {
    "text": "It is worth mentioning that the need for tooling support for logging also applies in an\nindustry setting. For instance, in a study conducted by Pecchia et al. (2015), they show that\nthe lack of format conventions in log messages, while not severe for manual analysis,\nundermines the use of automatic analysis. They suggest that a tool to detect inconsistent\nCândido et al. (2021), PeerJ Comput. Sci., DOI 10.7717/peerj-cs.489\n13/38\n\n\nconventions would be helpful for promptly ﬁxes. In a different study, Zhi et al. (2019)\nanalyses log conﬁgurations on 10 open-source projects and 10 Alibaba systems. They show\nthat developers often rely on logging conﬁgurations to control the throughput of data\nand quality of data (e.g., suppressing inconvenient logs generated from external\ndependencies, changing the layout format of the recorded events) but ﬁnding optimal\nsettings is challenging (observed as recurrent changes on development history).\nIn the context of mobile development, Zeng et al. (2019) show that logging practices are\ndifferent but developers still struggle with inconsistent logging.",
    "section_title": "Results",
    "section_level": 2,
    "page_number": 9,
    "chunk_index": 15,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Results",
      "section_level": 2
    }
  },
  {
    "text": "They observed a lower\ndensity of log statements compared to previous studies focused on server and desktop\nsystems (Chen & Jiang, 2017b; Yuan, Park & Zhou, 2012) by analyzing +1.4K Android\napps hosted on F-Droid. Logging practices in mobile development differ mainly because\ndevelopers need to consider the overhead impact on user’s device. The authors observed\na statistically signiﬁcant difference in terms of response time, battery consumption, and\nCPU when evaluating eight apps with logging enabled and disabled.\nUnderstanding the meaning of logs is important not only for analysis but also for\nmaintenance of logging code. However, one challenge that developers face is to actively\nupdate log-related code along functionalities. The code base naturally evolves but due to\nunawareness on how features are related to log statements, the latter become outdated\nand may produce misleading information (Yuan, Park & Zhou, 2012; Chen & Jiang,\n2017b). This is particularly problematic when the system is in production and developers\nneed to react for user inquiries. In this context, Shang et al.",
    "section_title": "Results",
    "section_level": 2,
    "page_number": 9,
    "chunk_index": 16,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Results",
      "section_level": 2
    }
  },
  {
    "text": "(2014) manually analyzed\nmailing lists and sampled log statements from three open-source projects (Apache\nHadoop, Zookeper, and Cassandra) to understand how practitioners and customers\nperceive log data. They highlight that common inquiries about log data relate to the\nmeaning, the cause, the context (e.g., in which cases a particular message appears in the log\nﬁles), the implications of a message, and solutions to manifested problems.\nIn a different study, Shang, Nagappan & Hassan (2015) investigated the relationship\nbetween logging code and the overall quality of the system though a case study on four\nreleases from Apache Hadoop and JBoss. They show that the presence of log statements\nare correlated to unstable source ﬁles and are strong indicators of defect-prone features.\nIn other words, classes that are more prone to defects often contain more logs.\nFinally, Kabinna et al. (2016) explored the reasons and the challenges of migrating to a\ndifferent logging library.",
    "section_title": "Results",
    "section_level": 2,
    "page_number": 9,
    "chunk_index": 17,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Results",
      "section_level": 2
    }
  },
  {
    "text": "The authors noticed that developers have different drivers for\nsuch a refactoring, e.g., to increase ﬂexibility, performance, and to reduce maintenance\neffort. Interestingly, the authors also observed that most projects suffer from post-\nmigration bugs because of the new logging library, and that migration rarely improved\nperformance.\nLog requirements\nAn important requirement of log data is that it must be informative and useful to a\nparticular purpose. Papers in this subcategory aim at evaluating whether log statements\ncan deliver expected data, given a known requirement.\nCândido et al. (2021), PeerJ Comput. Sci., DOI 10.7717/peerj-cs.489\n14/38\n\n\nFault injection is a technique that can be useful to assess the diagnosibility of log data,\ni.e., whether log data can manifest the presence of failures. Past studies conducted\nexperiments in open-source projects and show that logs are unable to produce any trace of\nfailures in most cases (Cinque et al., 2010; Pecchia & Russo, 2012; Cinque, Cotroneo &\nPecchia, 2013).",
    "section_title": "Results",
    "section_level": 2,
    "page_number": 9,
    "chunk_index": 18,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Results",
      "section_level": 2
    }
  },
  {
    "text": "The idea is to introduce faults in the system under test, run tests (these have\nto manifest failures), and compare the log data before and after the experiment.\nExamples of introduced faults are missing method calls and missing variable assignment.\nThe authors suggest the usage of fault injection as a guideline to identify and add missing\nlog statements.\nAnother approach to address the diagnosability in log data was proposed by Yuan et al.\n(2012). LOGENHANCER leverages program analysis techniques to capture additional context\nto enhance log statements in the execution ﬂow. Differently from past work with fault\ninjection, LOGENHANCER proposes the enhancement of existing log statements rather than\naddition of log statements in missing locations.\nIn the context of web services, da Cruz et al. (2004) already explored the idea of\nenhancing log data. An interesting remark pointed by the authors is that, in the context of\ncomplex system with third-party libraries, there is no ownership about the format and\ncontent of log statements.",
    "section_title": "Results",
    "section_level": 2,
    "page_number": 9,
    "chunk_index": 19,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Results",
      "section_level": 2
    }
  },
  {
    "text": "This is an issue if the log data generated is inappropriate and\nrequires changes (as observed by Zhi et al. (2019)). To overcome this issue, they propose\nWSLOG A, a logging framework based on SOAP intermediaries that intercepts messages\nexchanged between client and server and enhances web logs with important data for\nmonitoring and auditing, e.g., response and processing time.\nImplementation of log statements\nDevelopers need to make several decisions at development time that inﬂuence the quality\nof the generated log data. Past studies in logging practices show that in practice, developers\nrely on their own experience and logging is conducted in a trial-and-error manner in\nopen-source projects (Yuan, Park & Zhou, 2012; Chen & Jiang, 2017b) and industry\n(Pecchia et al., 2015). Papers in this subcategory aim at studying logging decisions,\ni.e., where to place log statements, which log level to use, and how to write log messages.\nHassani et al. (2018) proposed a set of checkers based in an empirical study of log-\nrelated changes in two open-source projects (Apache Hadoop and Apache Camel).",
    "section_title": "Results",
    "section_level": 2,
    "page_number": 9,
    "chunk_index": 20,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Results",
      "section_level": 2
    }
  },
  {
    "text": "They\nobserved that typos in log messages, missing guards (i.e., conditional execution of log\nstatement according to the appropriate level), and missing exception-related logging\n(e.g., unlogged exception or missing the exception in a log statement) are common causes\nfor code changes. Li et al. (2019a) also analyze log changes across several revisions on\n12 C/C++ open-source projects. However, they mine rules based on the type of\nmodiﬁcation (e.g., update on log descriptor) and contextual characteristics from the\nrevision. The rational is that new code changes with similar contextual characteristics\nshould have similar type of log modiﬁcation. The authors proposed this method in the\nform of a tool named LOGTRACKER. In another study, Chen & Jiang (2017a) analyzed 352\npairs of log-related changes from ActiveMQ, Hadoop, and Maven (all Apache projects),\nand proposed LCANALYZER, a checker that encodes the anti-patterns identiﬁed on their\nCândido et al. (2021), PeerJ Comput. Sci., DOI 10.7717/peerj-cs.489\n15/38\n\n\nanalysis.",
    "section_title": "Results",
    "section_level": 2,
    "page_number": 9,
    "chunk_index": 21,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Results",
      "section_level": 2
    }
  },
  {
    "text": "Some of these patterns are usage of nullable references, explicit type cast, and\nmalformed output (e.g., referencing data types without user-friendly string representation)\nin the log statement. Li et al. (2019b) addressed additional anti-patterns caused mainly\nby improper copy-and-paste, e.g., same log statement reused on different catch blocks.\nThey derived ﬁve duplication anti-patterns by studying 3K duplicated log statements on\nHadoop, ElasticSearch, CloudStack, and Cassandra, and encoded those anti-patterns in a\nchecker named DLFINDER. On the evaluation, they discovered not only new issues on the\nanalyzed systems but also on other two systems (Camel and Wicket). Note that several\nrecurrent problems aforementioned can be capture by static analysis before merging\nchanges into the code base.\nDeciding where to place log statements is critical to provide enough context for\nlater analysis. One way to identify missing locations is to use fault injection (see “Log\nRequirements”). However, the effectiveness of that approach is limited to the quality of\ntests and the ability of manifesting failures.",
    "section_title": "Results",
    "section_level": 2,
    "page_number": 9,
    "chunk_index": 22,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Results",
      "section_level": 2
    }
  },
  {
    "text": "Furthermore, log placement requires further\ncontextual information that is unfeasible to capture only with static analysis. Another\napproach to address consistent log placement in large code bases is to leverage source code\nanalysis and statistical models to mine log patterns. Fu et al. (2014a) conducted an\nempirical study in two Microsoft C# systems and proposed ﬁve classiﬁcations for log\nplacement: three for unexpected situations and two for regular monitoring. Unexpected\nsituations cover log statements triggered by failed assertions (“assertion logging”),\nexception handling or throw statements (“exception logging”), and return of unexpected\nvalues after a checking condition (“return-value-check logging”). Regular monitoring\ncover the remaining cases of log statements that can be in logic branches (“logic-branch\nlogging”) or not (“observing-point logging”). Later, Zhu et al.",
    "section_title": "Results",
    "section_level": 2,
    "page_number": 9,
    "chunk_index": 23,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Results",
      "section_level": 2
    }
  },
  {
    "text": "(2015) proposed\nLOGADVISOR, a technique that leverages supervised learning with feature engineering to\nsuggest log placement for unexpected situations, namely catch blocks (“exception\nlogging”) and if blocks with return statements (“return-value-check logging”). Some of\nthe features deﬁned for the machine learning process are size of the method, i.e., number of\nlines of source code, name of method parameters, name of local variables, and method\nsignature. They evaluated LOGADVISOR on two proprietary systems from Microsoft and\ntwo open-source projects hosted on GitHub. The results indicate the feasibility of applying\nmachine learning to provide recommendations for where to place new log statements.\nLi et al. (2018) approached the placement problem by correlating the presence of logging\ncode with the context of the source code. The rationale is that some contexts (deﬁned\nthrough topic models) are more likely to contain log statements (e.g., network or database\noperations) than others (e.g., getter methods).",
    "section_title": "Results",
    "section_level": 2,
    "page_number": 9,
    "chunk_index": 24,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Results",
      "section_level": 2
    }
  },
  {
    "text": "In this work, the authors analyze log\nplacement at method level rather than block-level as in previous work (Fu et al., 2014a;\nZhu et al., 2015).\nChoosing the appropriate severity level of log statements is a challenge. Recall that\nlogging frameworks provide the feature of suppressing log messages according to the log\nseverity. Li, Shang & Hassan (2017) proposed a machine learning-based technique to\nsuggest the log level of a new log statement. The underlying model uses ordinal regression,\nwhich is useful to predict classes, i.e., log level, but taking into account their severity order,\nCândido et al. (2021), PeerJ Comput. Sci., DOI 10.7717/peerj-cs.489\n16/38\n\n\ne.g., info < warning < error. Their technique provides better accuracy than random\nguessing and guessing based on the distribution of log levels in the source code. They\nreport that the log message and the surrounding context of the log statement are good\npredictors of the log level. It is worth mentioned that Hassani et al.",
    "section_title": "Results",
    "section_level": 2,
    "page_number": 9,
    "chunk_index": 25,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Results",
      "section_level": 2
    }
  },
  {
    "text": "(2018) also addressed\nthe problem of identifying appropriate log level in their study on log-related changes by\nexamining the entropy of log messages and log levels. The underlying idea is that log levels\nthat are commonly associated with a log message also should be used on other log\nstatements with similar log messages. While this approach is intuitive and precise, the\nauthors report low recall. Both studies highlight the relationship of the log message and\nassociate severity of a log statement. In another study, Anu et al. (2019) also proposes a\nclassiﬁer for log level recommendation. They focus on log statements located on if-else\nblocks and exception handling. In terms of feature engineering, the authors leverage\nmostly the terms associated in the code snippet (e.g., log message, code comments, and\nmethod calls) while Li, Shang & Hassan (2017) use quantitative metrics extracted from\ncode (e.g., length of log message and code complexity). However it remains open how both\ntechniques compare in terms of performance.\nAn important part of log statements is the description of the event being logged.",
    "section_title": "Results",
    "section_level": 2,
    "page_number": 9,
    "chunk_index": 26,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Results",
      "section_level": 2
    }
  },
  {
    "text": "Inappropriate descriptions are problematic and delay the analysis process. He et al. (2018a)\nconducted an empirical study focused on what developers log. They analyzed 17 projects\n(10 in Java and 7 in C#) and concluded that log descriptors are repetitive and small in\nvocabulary. For this reason, they suggest that it is feasible to exploit information retrieval\nmethods to automatically generate log descriptions.\nIn addition to log descriptors, the state of the system is another important information\nthe event being logged. Liu et al. (2019b) proposed a machine learning-based approach\nto aid developers about which variables to log based on the patterns of existing log\nstatements. The technique consists of four layers: embedding, Recurrent Neural Network\n(RNN), self-attention mechanism, and output. Results indicate better performance than\nrandom guess and information retrieve approaches on the evaluation of nine Java projects.",
    "section_title": "Results",
    "section_level": 2,
    "page_number": 9,
    "chunk_index": 27,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Results",
      "section_level": 2
    }
  },
  {
    "text": "Log infrastructure\nThe infrastructure supporting the analysis process plays an important role because\nthe analysis may involve the aggregation and selection of high volumes of data.\nThe requirements for the data processing infrastructure depend on the nature of the\nanalysis and the nature of the log data. For instance, popular log processors, e.g., Logstash\nand Fluentd, provide regular expressions out-of-the-box to extract data from well-\nknown log formats of popular web servers (e.g., Apache Tomcat and Nginx). However,\nextracting content from highly unstructured data into a meaningful schema is not trivial.\nLOG INFRASTRUCTURE deals with the tooling support necessary to make the further\nanalysis feasible. For instance, data representation might inﬂuence on the efﬁciency of data\naggregation. Other important concerns include the ability of handling log data for real-\ntime or ofﬂine analysis and scalability to handle the increasing volume of data.\nCândido et al. (2021), PeerJ Comput. Sci., DOI 10.7717/peerj-cs.489\n17/38\n\n\nWe observed two subcategories in this area: (1) log parsing, and (2) log storage.",
    "section_title": "Results",
    "section_level": 2,
    "page_number": 9,
    "chunk_index": 28,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Results",
      "section_level": 2
    }
  },
  {
    "text": "In the\nfollowing, we summarize the 16 studies on log infrastructure grouped by these two\ncategories.\nLog parsing\nParsing is the backbone of many log analysis techniques. Some analysis operate under the\nassumption that source-code is unavailable; therefore, they rely on parsing techniques to\nprocess log data. Given that log messages often have variable content, the main challenge\ntackled by these papers is to identify which log messages describe the same event. For\nexample, “Connection from A port B” and “Connection from C port D” represent the same\nevent. The heart of studies in parsing is the template extraction from raw log data.\nFundamentally, this process consists of identifying the constant and variable parts of raw\nlog messages.\nSeveral approaches rely on the “textual similarity” between the log messages. Aharon\net al. (2009) create a dictionary of all words that appear in the log message and use the\nfrequency of each word to cluster log messages together.",
    "section_title": "Results",
    "section_level": 2,
    "page_number": 9,
    "chunk_index": 29,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Results",
      "section_level": 2
    }
  },
  {
    "text": "Somewhat similar, IPLOM\n(Iterative Partitioning Log Mining) leverages the similarities between log messages related\nto the same event, e.g., number, position, and variability of tokens (Makanju, Zincir-\nHeywood & Milios, 2009; Makanju, Zincir-Heywood & Milios, 2012). Liang et al. (2007)\nalso build a dictionary out of the keywords that appear in the logs. Next, each log is\nconverted to a binary vector, with each element representing whether the log contains\nthat keyword. With these vectors, the authors compute the correlation between any two\nevents.\nSomewhat different from others, Gainaru et al. (2011) cluster log messages by searching\nfor the best place to split a log message into its “constant” and its “variable” parts. These\nclusters are self-adaptive as new log messages are processed in a streamed fashion.\nHamooni et al. (2016) also uses string similarity to cluster logs. Interestingly, authors\nhowever made use of map-reduce to speed up the processing. Finally, Zhou et al. (2010)\npropose a fuzzy match algorithm based on the contextual overlap between log lines.",
    "section_title": "Results",
    "section_level": 2,
    "page_number": 9,
    "chunk_index": 30,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Results",
      "section_level": 2
    }
  },
  {
    "text": "Transforming logs into “sequences” is another way of clustering logs. Lin et al.\n(2016) convert logs into vectors, where each vector contains a sequence of log events of a\ngiven task, and each event has a different weight, calculated in different ways. Tang &\nLi (2010) propose LOGTREE, a semi-structural way of representing a log message. The\noverall idea is to represent a log message as a tree, where each node is a token, extracted via\na context-free grammar parser that the authors wrote for each of the studied systems.\nInterestingly, in this paper, the authors raise awareness to the drawbacks of clustering\ntechniques that consider only word/term information for template extraction. According\nthem, log messages related to same events often do not share a single word.\nFrom an empirical perspective, He et al. (2016a) compared four log parsers on ﬁve\ndatasets with over 10 million raw log messages and evaluated their effectiveness in a\nreal log-mining task. The authors show, among many other ﬁndings, that current log\nparsing methods already achieve high accuracy, but do not scale well to large log data.",
    "section_title": "Results",
    "section_level": 2,
    "page_number": 9,
    "chunk_index": 31,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Results",
      "section_level": 2
    }
  },
  {
    "text": "Later, Zhu et al. (2019) extended the former study and evaluated a total of 13 parsing\nCândido et al. (2021), PeerJ Comput. Sci., DOI 10.7717/peerj-cs.489\n18/38\n\n\ntechniques on 16 datasets. In a different study, He et al. (2018b) also compared existing\nparsing techniques and proposed a distributed parsing technique for large-scale datasets\non top of Apache Spark. The authors show that for small datasets, the technique\nunderperforms due to the communication overhead between workers; however, for\nlarge-scale datasets (e.g., 200 million log messages), the approach overcomes traditional\ntechniques. It is worth mentioning that the large-scale datasets were synthetically\ngenerated on top of two popular datasets due to the lack of real-world datasets. Agrawal,\nKarlupia & Gupta (2019) also proposes a distributed approach based on Apache Spark for\ndistributed parsing. The comparison between the two approaches (He et al., 2018b;\nAgrawal, Karlupia & Gupta, 2019) remains open.\nLog storage\nModern complex systems easily generate giga- or petabytes of log data a day.",
    "section_title": "Results",
    "section_level": 2,
    "page_number": 9,
    "chunk_index": 32,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Results",
      "section_level": 2
    }
  },
  {
    "text": "Thus, in the\nlog data life-cycle, storage plays an important role as, when not handled carefully, it might\nbecome the bottleneck of the analysis process. Researchers and practitioners have been\naddressing this problem by ofﬂoading computation and storage to server farms and\nleveraging distributed processing.\nMavridis & Karatza (2017) frame the problem of log analysis at scale as a “big data”\nproblem. Authors evaluated the performance and resource usage of two popular big data\nsolutions (Apache Hadoop and Apache Spark) with web access logs. Their benchmarks\nshow that both approaches scale with the number of nodes in a cluster. However, Spark is\nmore efﬁcient for data processing since it minimizes reads and writes in disk. Results\nsuggest that Hadoop is better suited for ofﬂine analysis (i.e., batch processing) while Spark\nis better suited for online analysis (i.e., stream processing). Indeed, as mentioned early,\nHe et al. (2018b) leverages Spark for parallel parsing because of its fast in-memory\nprocessing.",
    "section_title": "Results",
    "section_level": 2,
    "page_number": 9,
    "chunk_index": 33,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Results",
      "section_level": 2
    }
  },
  {
    "text": "Another approach to reduce storage costs consists of data compression techniques for\nefﬁcient analysis (Lin et al., 2015; Liu et al., 2019a). Lin et al. (2015) argue that while\ntraditional data compression algorithms are useful to reduce storage footprint, the\ncompression-decompression loop to query data undermines the efﬁciency of log analysis.\nThe rationale is that traditional compression mechanisms (e.g., gzip) perform compression\nand decompression in blocks of data. In the context of log analysis, this results in waste\nof CPU cycles to compress and decompress unnecessary log data. They propose a\ncompression approach named Cowik that operates in the granularity of log entries.\nThey evaluated their approach in a log search and log joining system. Results suggest\nthat the approach is able to achieve better performance on query operations and produce\nthe same join results with less memory. Liu et al. (2019a) proposes a different approach\nnamed LOGZIP based on an intermediate representation of raw data that exploits the\nstructure of log messages.",
    "section_title": "Results",
    "section_level": 2,
    "page_number": 9,
    "chunk_index": 34,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Results",
      "section_level": 2
    }
  },
  {
    "text": "The underlying idea is to remove redundant information from\nlog events and compress the intermediate representation rather than raw logs. Results\nindicate higher compression rates compared to baseline approaches (including COWIK).\nCândido et al. (2021), PeerJ Comput. Sci., DOI 10.7717/peerj-cs.489\n19/38\n\n\nLog analysis\nAfter the processing of log data, the extracted information serves as input to sophisticated\nlog analysis methods and techniques. Such analysis, which make use of varying algorithms,\nhelp developers in detecting unexpected behavior, performance bottlenecks, or even\nsecurity problems.\nLOG ANALYSIS deals with knowledge acquisition from log data for a speciﬁc purpose,\ne.g., detecting undesired behavior or investigating the cause of a past outage. Extracting\ninsights from log data is challenging due to the complexity of the systems generating that\ndata.\nWe observed eight subcategories in this area: (1) anomaly detection, (2) security and\nprivacy, (3) root cause analysis, (4)",
    "section_title": "Results",
    "section_level": 2,
    "page_number": 9,
    "chunk_index": 35,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Results",
      "section_level": 2
    }
  },
  {
    "text": "failure prediction, (5) quality assurance, (6) model\ninference and invariant mining, (7) reliability and dependability, and (8) platforms. In the\nfollowing, we summarize the 68 studies on log analysis grouped by these seven different\ngoals.\nAnomaly detection\nAnomaly detection techniques aim to ﬁnd undesired patterns in log data given that\nmanual analysis is time-consuming, error-prone, and unfeasible in many cases.\nWe observe that a signiﬁcant part of the research in the logging area is focused on this type\nof analysis. Often, these techniques focus on identifying problems in software systems.\nBased on the assumption that an “anomaly” is something worth investigating, these\ntechniques look for anomalous traces in the log ﬁles.\nOliner & Stearley (2007) raise awareness on the need of datasets from real systems to\nconduct studies and provide directions to the research community.",
    "section_title": "Results",
    "section_level": 2,
    "page_number": 9,
    "chunk_index": 36,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Results",
      "section_level": 2
    }
  },
  {
    "text": "They analyzed log\ndata from ﬁve super computers and conclude that logs do not contain sufﬁcient\ninformation for automatic detection of failures nor root cause diagnosis, small events\nmight dramatically impact the number of logs generated, different failures have different\npredictive signatures, and messages that are corrupted or have inconsistent formats are\nnot uncommon. Many of the challenges raised by the authors are well known nowadays\nand have been in continuous investigation in academia.\nResearchers have been trying several different techniques, such as deep learning and\nNLP (Du et al., 2017; Bertero et al., 2017; Meng et al., 2019; Zhang et al., 2019), data mining,\nstatistical learning methods, and machine learning (Lu et al., 2017; He et al., 2016b;\nGhanbari, Hashemi & Amza, 2014; Tang & Iyer, 1992; Lim, Singh & Yajnik, 2008; Xu et al.,\n2009b, Xu et al., 2009a) control ﬂow graph mining from execution logs (Nandi et al., 2016),\nﬁnite state machines (Fu et al., 2009;",
    "section_title": "Results",
    "section_level": 2,
    "page_number": 9,
    "chunk_index": 37,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Results",
      "section_level": 2
    }
  },
  {
    "text": "Debnath et al., 2018), frequent itemset mining\n(Lim, Singh & Yajnik, 2008), dimensionality reduction techniques (Juvonen, Sipola &\nHämäläinen, 2015), grammar compression of log sequences (Gao et al., 2014), and\nprobabilistic sufﬁx trees (Bao et al., 2018).\nInterestingly, while these papers often make use of systems logs (e.g., logs generated\nby Hadoop, a common case study among log analysis in general) for evaluation, we\nconjecture that these approaches are sufﬁciently general, and could be explored in (or are\nworth trying at on) other types of logs (e.g., application logs).\nCândido et al. (2021), PeerJ Comput. Sci., DOI 10.7717/peerj-cs.489\n20/38\n\n\nResearchers have also explored log analysis techniques within speciﬁc contexts.\nFor instance, ﬁnding anomalies in HTTP logs by using dimensionality reduction\ntechniques (Juvonen, Sipola & Hämäläinen, 2015), ﬁnding anomalies in cloud operations\n(Farshchi et al., 2015; Farshchi et al., 2018) and Spark programs (Lu et al., 2017) by using\nmachine learning.",
    "section_title": "Results",
    "section_level": 2,
    "page_number": 9,
    "chunk_index": 38,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Results",
      "section_level": 2
    }
  },
  {
    "text": "As within many other ﬁelds of software engineering, we see an\nincreasingly adoption of machine and deep learning. In 2016, He et al. (2016b) then\nevaluated six different algorithms (three supervised, and three unsupervised machine\nlearning methods) for anomaly detection. The authors found that supervised anomaly\ndetection methods present higher accuracy when compared to unsupervised methods; that\nthe use of sliding windows (instead of a ﬁxed window) can increase the accuracy of the\nmethods; and that methods scale linearly with the log size. In 2017, Du et al. (2017)\nproposed DEEPLOG, a deep neural network model that used Long Short-Term Memory\n(LSTM) to model system logs as a natural language sequence, and Bertero et al. (2017)\nexplored the use of NLP, considering logs fully as regular text. In 2018, Debnath et al.\n(2018) (by means of the LOGMINE technique (Hamooni et al., 2016)) explored the use of\nclustering and pattern matching techniques. In 2019, Meng et al. (2019) proposed a\ntechnique based on unsupervised learning for unstructured data.",
    "section_title": "Results",
    "section_level": 2,
    "page_number": 9,
    "chunk_index": 39,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Results",
      "section_level": 2
    }
  },
  {
    "text": "It features a transformer\nTEMPLATE2VEC (as an alternative to WORD2VEC) to represent extracted templates from\nlogs and LSTMs to learn common sequences of log sequences. In addition, Zhang et al.\n(2019) leverages LSTM models with attention mechanism to handle unstable log data.\nThey argue that log data changes over time due to evolution of software and models\naddressing log analysis need to take this into consideration.\nSecurity and privacy\nLogs can be leveraged for security purposes, such as intrusion and attacks detection.\nOprea et al. (2015) use (web) trafﬁc logs to detect early-stage malware and advanced\npersistence threat infections in enterprise network, by modeling the information based on\nbelief propagation inspired by graph theory. Chu et al. (2012) analyses access logs (in their\ncase, from TACACS+, an authentication protocol developed by Cisco) to distinguish\nnormal operational activities from rogue/anomalous ones.",
    "section_title": "Results",
    "section_level": 2,
    "page_number": 9,
    "chunk_index": 40,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Results",
      "section_level": 2
    }
  },
  {
    "text": "Yoon & Squicciarini (2014)\nfocus on the analysis and detection of attacks launched by malicious or misconﬁgured\nnodes, which may tamper with the ordinary functions of the MapReduce framework.\nYen et al. (2013) propose Beehive, a large-scale log analysis for detecting suspicious activity\nin enterprise networks, based on logs generated by various network devices. In the\ntelecommunication context, Goncalves, Bota & Correia (2015) used clustering algorithms\nto identify malicious activities based on log data from ﬁrewall, authentication and DHCP\nservers.\nAn interesting characteristic among them all is that the most used log data is, by far,\nnetwork data. We conjecture this is due to the fact that (1) network logs (e.g., HTTP, web,\nrouter logs) are independent from the underlying application, and that (2) network\ntends to be, nowadays, a common way of attacking an application.\nDifferently from analysis techniques where the goal is to ﬁnd a bug, and which are\nrepresented in the logs as anomalies, understanding which characteristics of log messages\nCândido et al. (2021), PeerJ Comput.",
    "section_title": "Results",
    "section_level": 2,
    "page_number": 9,
    "chunk_index": 41,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Results",
      "section_level": 2
    }
  },
  {
    "text": "Sci., DOI 10.7717/peerj-cs.489\n21/38\n\n\ncan reveal security issues is still an open topic. Barse & Jonsson (2004) extract attack\nmanifestations to determine log data requirements for intrusion detection. The authors\nthen present a framework for determining empirically which log data can reveal a\nspeciﬁc attack. Similarly, Abad et al. (2003) argue for the need of correlation data from\ndifferent logs to improve the accuracy of intrusion detection systems. The authors show in\ntheir paper how different attacks are reﬂected in different logs, and how some attacks are\nnot evident when analyzing single logs. Prewett (2005) examines how the unique\ncharacteristics of cluster machines, including how they are generally operated in the\nlarger context of a computing center, can be leveraged to provide better security.\nFinally, regarding privacy, Butin & Le Métayer (2014) propose a framework for\naccountability based on “privacy-friendly” event logs. These logs are then used to show\ncompliance with respect to data protection policies.",
    "section_title": "Results",
    "section_level": 2,
    "page_number": 9,
    "chunk_index": 42,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Results",
      "section_level": 2
    }
  },
  {
    "text": "Root cause analysis\nDetecting anomalous behavior, either by automatic or monitoring solutions, is just part of\nthe process. Maintainers need to investigate what caused that unexpected behavior. Several\nstudies attempt to take the next step and provide users with, e.g., root cause analysis,\naccurate failure identiﬁcation, and impact analysis.\nKimura et al. (2014) identify spatial-temporal patterns in network events. The authors\nafﬁrm that such spatial-temporal patterns can provide useful insights on the impact\nand root cause of hidden network events. Ren et al. (2019) explores a similar idea in\nthe context of diagnosing non-reproducible builds. They propose a differential analysis\namong different build traces based on I/O and parent-child dependencies. The technique\nleverages the common dependencies patterns to ﬁlter abnormal patterns and to pinpoint\nthe cause of the non-reproducible build. Pi et al. (2018) propose a feedback control\ntool for distributed applications in virtualized environments.",
    "section_title": "Results",
    "section_level": 2,
    "page_number": 9,
    "chunk_index": 43,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Results",
      "section_level": 2
    }
  },
  {
    "text": "By correlating log messages\nand resource consumption, their approach builds relationships between changes in\nresource consumption and application events. Somewhat related, Chuah et al. (2013)\nidentiﬁes anomalies in resource usage, and link such anomalies to software failures. Zheng\net al. (2011) also argue for the need of correlating different log sources for a better problem\nidentiﬁcation. In their study, authors correlate supercomputer BlueGene’s reliability,\navailability and serviceability logs with its job logs, and show that such a correlation was\nable to identify several important observations about why their systems and jobs fail.\nGurumdimma et al. (2016) also leverages multiple sources of data for accurate diagnosis of\nmalfunctioning nodes in the Ranger Supercomputer. The authors argue that, while console\nlogs are useful for administration tasks, they can complex to analyze by operators.\nThey propose a technique based on the correlation of console logs and resource usage\ninformation to link jobs with anomalous behavior and erroneous nodes.",
    "section_title": "Results",
    "section_level": 2,
    "page_number": 9,
    "chunk_index": 44,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Results",
      "section_level": 2
    }
  },
  {
    "text": "Failure prediction\nBeing able to anticipate failures in critical systems not only represents competitive business\nadvantage but also represents prevention of unrecoverable consequences to the business.\nCândido et al. (2021), PeerJ Comput. Sci., DOI 10.7717/peerj-cs.489\n22/38\n\n\nFailure prediction is feasible once there is knowledge about abnormal patterns and\ntheir related causes. However, it differs from anomaly detection in the sense that\nidentifying the preceding patterns of an unrecoverable state requires insights from root\ncause analysis. This approach shifts monitoring to a proactive manner rather than reactive,\ni.e., once the problem occurred.\nWork in this area, as expected, relies on statistical and probabilistic models, from\nstandard regression analysis to machine learning. Wang et al. (2017) apply random forests\nin event logs to predict maintenance of equipment (in their case study, ATMs). Fu et al.\n(2014b) use system logs (from clusters) to generate causal dependency graphs and\npredict failures.",
    "section_title": "Results",
    "section_level": 2,
    "page_number": 9,
    "chunk_index": 45,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Results",
      "section_level": 2
    }
  },
  {
    "text": "Russo, Succi & Pedrycz (2015) mine system logs (more speciﬁcally,\nsequences of logs) to predict the system’s reliability by means of linear radial basis\nfunctions, and multi-layer perceptron learners. Khatuya et al. (2018) propose ADELE, a\nmachine learning-based technique to predict functional and performance issues. Shalan &\nZulkernine (2013) utilize system logs to predict failure occurrences by means of regression\nanalysis and support vector machines. Fu et al. (2012) also utilize system logs to\npredict failures by mining recurring event sequences that are correlated.\nWe noticed that, given that only supervised models have been used so far, feature\nengineering plays an important role in these papers. Khatuya et al. (2018), for example,\nuses event count, event ratio, mean inter-arrival time, mean inter-arrival distance, severity\nspread, and time-interval spread. Russo, Succi & Pedrycz (2015) use defective and non\ndefective sequences of events as features.",
    "section_title": "Results",
    "section_level": 2,
    "page_number": 9,
    "chunk_index": 46,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Results",
      "section_level": 2
    }
  },
  {
    "text": "Shalan & Zulkernine (2013)’s paper, although not\ncompletely explicit about which features they used, mention CPU, memory utilization,\nread/write instructions, error counter, error messages, error types, and error state\nparameters as examples of features.\nQuality assurance\nLog analysis might support developers during the software development life cycle and,\nmore speciﬁcally, during activities related to quality assurance.\nAndrews & Zhang (2000, 2003) advocated the use of logs for testing purposes since the\nearly 2000’s. In their work, the authors propose an approach called log ﬁle analysis (LFA).\nLFA requires the software under test to write a record of events to a log ﬁle, following a\npre-deﬁned logging policy that states precisely what the software should log. A log ﬁle\nanalyzer, also written by the developers, then analyses the produced log ﬁle and only\naccepts it in case the run did not reveal any failures. The authors propose a log ﬁle analysis\nlanguage to specify such analyses.\nMore than 10 years later, Chen et al.",
    "section_title": "Results",
    "section_level": 2,
    "page_number": 9,
    "chunk_index": 47,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Results",
      "section_level": 2
    }
  },
  {
    "text": "(2018) propose an automated approach to estimate\ncode coverage via execution logs named LogCoCo. The motivation for this use of log\ndata comes from the need to estimate code coverage from production code. The authors\nargue that, in a large-scale production system, code coverage from test workloads might\nnot reﬂect coverage under production workload. Their approach relies on program\nanalysis techniques to match log data and their corresponding code paths. Based on this\ndata, LogCoCo estimates different coverage criteria, i.e., method, statement, and branch\nCândido et al. (2021), PeerJ Comput. Sci., DOI 10.7717/peerj-cs.489\n23/38\n\n\ncoverage. Their experiments in six different systems show that their approach is highly\naccurate (>96%).\nModel inference and invariant mining\nModel-based approaches to software engineering seek to support understanding and\nanalysis by means of abstraction. However, building such models is a challenging and\nexpensive task. Logs serve as a source for developers to build representative models and\ninvariants of their systems.",
    "section_title": "Results",
    "section_level": 2,
    "page_number": 9,
    "chunk_index": 48,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Results",
      "section_level": 2
    }
  },
  {
    "text": "These models and invariants may help developers in\ndifferent tasks, such as comprehensibility and testing. These approaches generate different\ntypes of models, such as (ﬁnite) state machines (Ulrich et al., 2003; Mariani & Pastore,\n2008; Tan et al., 2010; Beschastnikh et al., 2014) directed workﬂow graphs (Wu, Anchuri &\nLi, 2017) client-server interaction diagrams (Awad & Menasce, 2016), invariants (Kc & Gu,\n2011; Lou et al., 2010), and dependency models (Steinle et al., 2006).\nState machines are the most common type of model extracted from logs. Beschastnikh\net al. (2014), for example, infer state machine models of concurrent systems from logs.\nThe authors show that their models are sufﬁciently accurate to help developers in ﬁnding\nbugs. Ulrich et al. (2003) show how log traces can be used to build formal execution\nmodels. The authors use SDL, a model-checking description technique, common in\ntelecommunication industries. Mariani & Pastore (2008) propose an approach where state\nmachine-based models of valid behaviors are compared with log traces of failing\nexecutions.",
    "section_title": "Results",
    "section_level": 2,
    "page_number": 9,
    "chunk_index": 49,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Results",
      "section_level": 2
    }
  },
  {
    "text": "The models are inferred via the kBehavior engine (Mariani & Pastore, 2008).\nTan et al. (2010) extract state-machine views of the MapReduce ﬂow behavior using the\nnative logs that Hadoop MapReduce systems produce.\nThe mining of properties that a system should hold has also been possible via log\nanalysis. Lou et al. (2010) derive program invariants from logs. The authors show that the\ninvariants that emerge from their approach are able to detect numerous real-world\nproblems. Kc & Gu (2011) aim to facilitate the troubleshooting of cloud computing\ninfrastructures. Besides implementing anomaly detection techniques, their tool also\nperforms invariant checks in log events, e.g., two processes performing the same task at the\nsame time (these invariants are not automatically devised, but should be written by system\nadministrators).\nWe also observe directed workﬂow graphs and dependency maps as other types of\nmodels built from logs.",
    "section_title": "Results",
    "section_level": 2,
    "page_number": 9,
    "chunk_index": 50,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Results",
      "section_level": 2
    }
  },
  {
    "text": "Wu, Anchuri & Li (2017) propose a method that mines structural\nevents and transforms them into a directed workﬂow graph, where nodes represent log\npatterns, and edges represent the relations among patterns. Awad & Menasce (2016)\nderive performance models of operational systems based on system logs and conﬁguration\nlogs. Finally, Steinle et al. (2006) map dependencies among internal components through\nsystem logs, via data mining algorithms and natural language processing techniques.\nFinally, and somewhat different from the other papers in this ramiﬁcation, Di Martino,\nCinque & Cotroneo (2012) argue that an important issue in log analysis is that, when a\nfailure happens, multiple independent error events appear in the log. Reconstructing\nthe failure process by grouping together events related to the same failure (also known as\ndata coalescence techniques) can therefore help developers in ﬁnding the problem.\nCândido et al. (2021), PeerJ Comput. Sci., DOI 10.7717/peerj-cs.489\n24/38\n\n\nAccording to the authors, while several coalescence techniques have been proposed over\ntime (Tsao & Siewiorek, 1983;",
    "section_title": "Results",
    "section_level": 2,
    "page_number": 9,
    "chunk_index": 51,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Results",
      "section_level": 2
    }
  },
  {
    "text": "Hansen & Siewiorek, 1992), evaluating these approaches is\na challenging task as the ground truth of the failure is often not available. To help\nresearchers in evaluating their approaches, the authors propose a technique which\nbasically generates synthetic logs along with the ground truth they represent.\nReliability and dependability\nLogs can serve as a means to estimate how reliable and dependable a software system is.\nResearch in this subcategory often focuses on large software systems, such as web and\nmobile applications that are distributed in general, and high performance computers.\nBanerjee, Srikanth & Cukic (2010) estimate the reliability of a web Software-as-a-Service\n(SaaS) by analyzing its web trafﬁc logs. Authors categorize different types of log events\nwith different severity levels, counting, e.g, successfully loaded (non-critical) images\nseparately from core transactions, providing different perspectives on reliability.",
    "section_title": "Results",
    "section_level": 2,
    "page_number": 9,
    "chunk_index": 52,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Results",
      "section_level": 2
    }
  },
  {
    "text": "Tian,\nRudraraju & Li (2004) evaluate the reliability of two web applications, using several\nmetrics that can be extracted from web access and error logs (e.g., errors per page hits,\nerrors per sessions, and errors per users). The authors conclude that the usage of workload\nand usage patterns, present in log ﬁles, during testing phases could signiﬁcantly improve\nthe reliability of the system. Later, Huynh & Miller (2009) expanded previous work\n(Tian, Rudraraju & Li, 2004) by enumerating improvements for reliability assessment.\nThey emphasize that some (http) error codes require a more in-depth analysis, e.g., errors\ncaused by factors that cannot be controlled by the website administrators should be\nseparated from the ones that can be controlled, and that using IP addresses as a way to\nmeasure user count can be misleading, as often many users share the same IP address.\nOutside the web domain, El-Sayed & Schroeder (2013) explore a decade of ﬁeld data\nfrom the Los Alamos National Lab and",
    "section_title": "Results",
    "section_level": 2,
    "page_number": 9,
    "chunk_index": 53,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Results",
      "section_level": 2
    }
  },
  {
    "text": "study the impact of different factors, such as\npower quality, temperature, fan activity, system usage, and even external factors, such\nas cosmic radiation, and their correlation with the reliability of High Performance\nComputing (HPC) systems. Among the lessons learned, the authors observe that the\nday following a failure, a node is 5 to 20 times more likely to experience an additional\nfailure, and that power outages not only increase follow-up software failures, but also\ninfrastructure failures, such as problems in distributed storage and ﬁle systems. In a later\nstudy, Park et al. (2017) discuss the challenges of analyzing HPC logs. Log analysis of HPC\ndata requires understanding underlying hardware characteristics and demands processing\nresources to analyze and correlate data. The authors introduce an analytic framework\nbased on NOSQL databases and Big Data technology (Spark) for efﬁcient in-memory\nprocessing to assist system administrators.\nAnalyzing the performance of mobile applications can be challenging specially when\nthey depend on back-end distributed services.",
    "section_title": "Results",
    "section_level": 2,
    "page_number": 9,
    "chunk_index": 54,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Results",
      "section_level": 2
    }
  },
  {
    "text": "IBM researchers (Ramakrishna et al., 2017)\nproposed MIAS (Mobile Infrastructure Analytics System) to analyze performance of\nmobile applications. The technique considers session data and system logs from\ninstrumented applications and back-end services (i.e., servers and databases) and applies\nCândido et al. (2021), PeerJ Comput. Sci., DOI 10.7717/peerj-cs.489\n25/38\n\n\nstatistical methods to correlate them and reduce the size of relevant log data for further\nanalysis.\nLog platforms\nMonitoring systems often contain dashboards and metrics to measure the “heartbeat” of\nthe system. In the occurrence of abnormal behavior, the operations team is able to visualize\nthe abnormality and conduct further investigation to identify the cause. Techniques to\nreduce/ﬁlter the amount of log data and efﬁcient querying play an important role to\nsupport the operations team on diagnosing problems. One consideration is, while visual\naid is useful, in one extreme, it can be overwhelming to handle several charts and\ndashboards at once.",
    "section_title": "Results",
    "section_level": 2,
    "page_number": 9,
    "chunk_index": 55,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Results",
      "section_level": 2
    }
  },
  {
    "text": "In addition, it can be non-trivial to judge if an unknown pattern on\nthe dashboard represents an unexpected situation. In practice, operations engineers may\nrely on experience and past situations to make this judgment. Papers in this subcategory\nfocus on full-ﬂedged platforms that aim at providing a full experience for monitoring\nteams.\nTwo studies were explicitly conducted in an industry setting, namely MELODY\n(Aharoni et al., 2011) at IBM and FLAP (Li et al., 2017) at Huawei Technologies. MELODY\nis a tool for efﬁcient log mining that features machine learning-based anomaly detection\nfor proactive monitoring. It was applied with ten large IBM clients, and the authors\nreported that MELODY was useful to reduce the excessive amount of data faced by their\nusers. FLAP is a tool that combines state-of-the-art processing, storage, and analysis\ntechniques. One interesting feature that was not mentioned in other studies is the use of\ntemplate learning for unstructured logs. The authors also report that FLAP is in\nproduction internally at Huawei.",
    "section_title": "Results",
    "section_level": 2,
    "page_number": 9,
    "chunk_index": 56,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Results",
      "section_level": 2
    }
  },
  {
    "text": "While an industry setting is not always accessible to the research community, publicly\navailable datasets are useful to overcome this limitation. Balliu et al. (2015) propose BIDAL,\na tool to characterize the workload of cloud infrastructures, They use log data from Google\ndata clusters for evaluation and incorporate support to popular analysis languages and\nstorage backends on their tool. Di et al. (2017) propose LOGAIDER, a tool that integrates log\nmining and visualization to analyze different types of correlation (e.g., spatial and\ntemporal). In this study, they use log data from Mira, an IBM Blue Gene-based\nsupercomputer for scientiﬁc computing, and reported high accuracy and precision in\nuncovering correlations associated with failures. Gunter et al. (2007) propose a log\nsummarization solution for time-series data integrated with anomaly detection techniques\nto troubleshoot grid systems. They used a publicly available testbed and conducted\ncontrolled experiments to generate log data and anomalous events.",
    "section_title": "Results",
    "section_level": 2,
    "page_number": 9,
    "chunk_index": 57,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Results",
      "section_level": 2
    }
  },
  {
    "text": "The authors highlight\nthe importance of being able to choose which anomaly detection technique to use, since\nthey observed different performance depending on the anomaly under analysis.\nOpen-source systems for cloud infrastructure and big data can be also used as\nrepresentative objects of study. Yu et al. (2016) and Neves, Machado & Pereira (2018)\nconduct experiments based on OpenStack and Apache Zookeeper, respectively. CLOUDSEER\n(Yu et al., 2016) is a solution to monitor management tasks in cloud infrastructures.\nThe technique is based on the characterization of administrative tasks as models inferred\nCândido et al. (2021), PeerJ Comput. Sci., DOI 10.7717/peerj-cs.489\n26/38\n\n\nfrom logs. CloudSeer reports anomalies based on model deviation and aggregates\nassociated logs for further inspection. Finally, FALCON (Neves, Machado & Pereira, 2018)\nis a tool that builds space-time diagrams from log data. It features a happens-before\nsymbolic modeling that allows obtaining ordered event scheduling from unsynchronized\nmachines.",
    "section_title": "Results",
    "section_level": 2,
    "page_number": 9,
    "chunk_index": 58,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Results",
      "section_level": 2
    }
  },
  {
    "text": "One interesting feature highlighted by the authors is the modular design of tool\nfor ease extension.\nDISCUSSION\nOur results show that logging is an active research ﬁeld that attracted not only researchers\nbut also practitioners. We observed that most of the research effort focuses on log analysis\ntechniques, while the other research areas are still in a early stage. In the following, we\nhighlight open problems, gaps, and future directions per research area.\nIn LOGGING, several empirical studies highlight the importance of better tooling\nsupport for developers since logging is conducted in a trial-and-error manner\n(see subcategory “Empirical Studies”). Part of the problem is the lack of requirements\nfor log data. When the requirements are well deﬁned, logging frameworks can be tailored\nto a particular use case and it is feasible to test whether the generated log data ﬁts the use\ncase (see subcategory “Log Requirements”). However, when requirements are not clear,\ndevelopers rely on their own experience to make log-related decisions.",
    "section_title": "Results",
    "section_level": 2,
    "page_number": 9,
    "chunk_index": 59,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Results",
      "section_level": 2
    }
  },
  {
    "text": "While static analysis\nis useful to anticipate potential issues in log statements (e.g., null reference in a logged\nvariable), other logging decisions (e.g., where to log) rely on the context of source code\n(see subcategory “Implementation of Log Statements”). Research on this area already\nshows the feasibility of employing machine learning to address those context-sensitive\ndecisions. However, it is still unknown the implications of deploying such tools to\ndevelopers. Further work is necessary to address usability and operational aspects of those\ntechniques. For instance, false positives is a reality in machine learning. There is no 100%\naccurate model and false positives will eventually emerge even if in a low rate. How to\ncommunicate results in a way that developers keeps engaged in a productive way is\nimportant to bridge the gap of theory and practice. This also calls for closer collaboration\nbetween academia and industry.\nIn LOG INFRASTRUCTURE, most of the research effort focused on parsing techniques.",
    "section_title": "Results",
    "section_level": 2,
    "page_number": 9,
    "chunk_index": 60,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Results",
      "section_level": 2
    }
  },
  {
    "text": "We observed that most papers in the “Log Parsing” subcategory address the template\nextraction problem as an unsupervised problem, mainly by clustering the static part of the\nlog messages. While the analysis of system logs (e.g., web logs and other data provided that\nthe runtime environment) was extensively explored (mostly Hadoop log data), little has\nbeen explored in the ﬁeld of application logs. We believe that this is due to the lack of\npublicly available dataset. In addition, application logs might not have a well-deﬁned\nstructure and can vary signiﬁcantly from structured system logs. This could undermine\nthe feasibility of exploiting clustering techniques. One way to address the availability\nproblem could be using log data generated from test suites in open-source projects.\nHowever, test suites might not produce comparable volume of data. Unless there is a\npublicly available large-scale application that could be used by the research community, we\nargue that the only way to explore log parsing at large-scale is in partnership with industry.\nCândido et al. (2021), PeerJ Comput.",
    "section_title": "Results",
    "section_level": 2,
    "page_number": 9,
    "chunk_index": 61,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Results",
      "section_level": 2
    }
  },
  {
    "text": "from logs. CloudSeer reports anomalies based on model deviation and aggregates\nassociated logs for further inspection. Finally, FALCON (Neves, Machado & Pereira, 2018)\nis a tool that builds space-time diagrams from log data. It features a happens-before\nsymbolic modeling that allows obtaining ordered event scheduling from unsynchronized\nmachines. One interesting feature highlighted by the authors is the modular design of tool\nfor ease extension.\nDISCUSSION\nOur results show that logging is an active research ﬁeld that attracted not only researchers\nbut also practitioners. We observed that most of the research effort focuses on log analysis\ntechniques, while the other research areas are still in a early stage. In the following, we\nhighlight open problems, gaps, and future directions per research area.\nIn LOGGING, several empirical studies highlight the importance of better tooling\nsupport for developers since logging is conducted in a trial-and-error manner\n(see subcategory “Empirical Studies”). Part of the problem is the lack of requirements\nfor log data.",
    "section_title": "Discussion",
    "section_level": 2,
    "page_number": 27,
    "chunk_index": 0,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Discussion",
      "section_level": 2
    }
  },
  {
    "text": "When the requirements are well deﬁned, logging frameworks can be tailored\nto a particular use case and it is feasible to test whether the generated log data ﬁts the use\ncase (see subcategory “Log Requirements”). However, when requirements are not clear,\ndevelopers rely on their own experience to make log-related decisions. While static analysis\nis useful to anticipate potential issues in log statements (e.g., null reference in a logged\nvariable), other logging decisions (e.g., where to log) rely on the context of source code\n(see subcategory “Implementation of Log Statements”). Research on this area already\nshows the feasibility of employing machine learning to address those context-sensitive\ndecisions. However, it is still unknown the implications of deploying such tools to\ndevelopers. Further work is necessary to address usability and operational aspects of those\ntechniques. For instance, false positives is a reality in machine learning. There is no 100%\naccurate model and false positives will eventually emerge even if in a low rate.",
    "section_title": "Discussion",
    "section_level": 2,
    "page_number": 27,
    "chunk_index": 1,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Discussion",
      "section_level": 2
    }
  },
  {
    "text": "How to\ncommunicate results in a way that developers keeps engaged in a productive way is\nimportant to bridge the gap of theory and practice. This also calls for closer collaboration\nbetween academia and industry.\nIn LOG INFRASTRUCTURE, most of the research effort focused on parsing techniques.\nWe observed that most papers in the “Log Parsing” subcategory address the template\nextraction problem as an unsupervised problem, mainly by clustering the static part of the\nlog messages. While the analysis of system logs (e.g., web logs and other data provided that\nthe runtime environment) was extensively explored (mostly Hadoop log data), little has\nbeen explored in the ﬁeld of application logs. We believe that this is due to the lack of\npublicly available dataset. In addition, application logs might not have a well-deﬁned\nstructure and can vary signiﬁcantly from structured system logs. This could undermine\nthe feasibility of exploiting clustering techniques. One way to address the availability\nproblem could be using log data generated from test suites in open-source projects.",
    "section_title": "Discussion",
    "section_level": 2,
    "page_number": 27,
    "chunk_index": 2,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Discussion",
      "section_level": 2
    }
  },
  {
    "text": "However, test suites might not produce comparable volume of data. Unless there is a\npublicly available large-scale application that could be used by the research community, we\nargue that the only way to explore log parsing at large-scale is in partnership with industry.\nCândido et al. (2021), PeerJ Comput. Sci., DOI 10.7717/peerj-cs.489\n27/38\n\n\nIndustry would highly beneﬁt from this collaboration, as researchers would be able to\nexplore latest techniques under a real workload environment. In addition to the\nexploration of application logs, there are other research opportunities for log parsing. Most\npapers exploit parsing for log analysis tasks. While this is an important application with its\nown challenges (e.g., data labeling), parsing could be also applied for efﬁcient log\ncompression and better data storage.\nLOG ANALYSIS is the research area with the highest number of primary studies, and our\nstudy shows that the body of knowledge for data modeling and analysis is already\nextensive. For instance, logs can be viewed as sequences of events, count vectors, or graphs.",
    "section_title": "Discussion",
    "section_level": 2,
    "page_number": 27,
    "chunk_index": 3,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Discussion",
      "section_level": 2
    }
  },
  {
    "text": "Each representation enables the usage of different algorithms that might outperform other\napproaches under different circumstances. However, it remains open how different\napproaches compare to each other. To fulﬁll this gap, future research must address what\ntrade-offs to apply and elaborate on the circumstances that make one approach more\nsuitable than the other. A public repository on GitHub (Loghub: https://github.com/\nlogpai/loghub) contains several datasets used in many studies in log analysis. We\nencourage practitioners and researchers to contribute to this collective effort. In addition,\nmost papers frame a log analysis task as a supervised learning problem. While this is the\nmost popular approach for machine learning, the lack of representative datasets with\nlabeled data is an inherent barrier. Projects operating in a continuous delivery culture,\nwhere software changes at a fast pace (e.g., hourly deploys), training data might become\noutdated quickly and the cost of collecting and labeling new data might be prohibitive.",
    "section_title": "Discussion",
    "section_level": 2,
    "page_number": 27,
    "chunk_index": 4,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Discussion",
      "section_level": 2
    }
  },
  {
    "text": "We suggest researchers to also consider how their techniques behave in such dynamic\nenvironment. More speciﬁcally, future work could explore the use of semi-supervised and\nunsupervised learning to overcome the cost of creating and updating datasets.\nTHREATS TO VALIDITY\nOur study maps the research landscape in logging, log infrastructure, and log analysis\nbased on our interpretation of the 108 studies published from 1992 to 2019. In this section,\nwe discuss possible threats to the validity of this work and possibilities for future\nexpansions of this systematic mapping.\nExternal validity\nThe main threat to the generalization of our conclusions relates to the representativeness\nof our dataset. Our procedure to discover relevant papers consists of querying popular\ndigital libraries rather than looking into already known venues in Software Engineering\n(authors’ ﬁeld of expertise). While we collected data from ﬁve different sources, it is\nunclear how each library indexes the entries. It is possible that we may have missed a\nrelevant paper because none of the digital libraries reported it.",
    "section_title": "Discussion",
    "section_level": 2,
    "page_number": 27,
    "chunk_index": 5,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Discussion",
      "section_level": 2
    }
  },
  {
    "text": "Therefore, the search\nprocedure might be unable to yield complete results. Another factor that inﬂuences the\ncompleteness of our dataset is the ﬁltering of papers based on the venue rank (i.e., A and A*\naccording to the CORE Rank). There are several external factors that inﬂuence the\nacceptance of a paper that are not necessarily related to the quality and relevance of the\nstudy. The rationale for applying the exclusion criterion by venue rank is to reduce the\nCândido et al. (2021), PeerJ Comput. Sci., DOI 10.7717/peerj-cs.489\n28/38",
    "section_title": "Discussion",
    "section_level": 2,
    "page_number": 27,
    "chunk_index": 6,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Discussion",
      "section_level": 2
    }
  },
  {
    "text": "Industry would highly beneﬁt from this collaboration, as researchers would be able to\nexplore latest techniques under a real workload environment. In addition to the\nexploration of application logs, there are other research opportunities for log parsing. Most\npapers exploit parsing for log analysis tasks. While this is an important application with its\nown challenges (e.g., data labeling), parsing could be also applied for efﬁcient log\ncompression and better data storage.\nLOG ANALYSIS is the research area with the highest number of primary studies, and our\nstudy shows that the body of knowledge for data modeling and analysis is already\nextensive. For instance, logs can be viewed as sequences of events, count vectors, or graphs.\nEach representation enables the usage of different algorithms that might outperform other\napproaches under different circumstances. However, it remains open how different\napproaches compare to each other. To fulﬁll this gap, future research must address what\ntrade-offs to apply and elaborate on the circumstances that make one approach more\nsuitable than the other.",
    "section_title": "Threats to validity",
    "section_level": 2,
    "page_number": 28,
    "chunk_index": 0,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Threats to validity",
      "section_level": 2
    }
  },
  {
    "text": "A public repository on GitHub (Loghub: https://github.com/\nlogpai/loghub) contains several datasets used in many studies in log analysis. We\nencourage practitioners and researchers to contribute to this collective effort. In addition,\nmost papers frame a log analysis task as a supervised learning problem. While this is the\nmost popular approach for machine learning, the lack of representative datasets with\nlabeled data is an inherent barrier. Projects operating in a continuous delivery culture,\nwhere software changes at a fast pace (e.g., hourly deploys), training data might become\noutdated quickly and the cost of collecting and labeling new data might be prohibitive.\nWe suggest researchers to also consider how their techniques behave in such dynamic\nenvironment. More speciﬁcally, future work could explore the use of semi-supervised and\nunsupervised learning to overcome the cost of creating and updating datasets.\nTHREATS TO VALIDITY\nOur study maps the research landscape in logging, log infrastructure, and log analysis\nbased on our interpretation of the 108 studies published from 1992 to 2019.",
    "section_title": "Threats to validity",
    "section_level": 2,
    "page_number": 28,
    "chunk_index": 1,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Threats to validity",
      "section_level": 2
    }
  },
  {
    "text": "In this section,\nwe discuss possible threats to the validity of this work and possibilities for future\nexpansions of this systematic mapping.\nExternal validity\nThe main threat to the generalization of our conclusions relates to the representativeness\nof our dataset. Our procedure to discover relevant papers consists of querying popular\ndigital libraries rather than looking into already known venues in Software Engineering\n(authors’ ﬁeld of expertise). While we collected data from ﬁve different sources, it is\nunclear how each library indexes the entries. It is possible that we may have missed a\nrelevant paper because none of the digital libraries reported it. Therefore, the search\nprocedure might be unable to yield complete results. Another factor that inﬂuences the\ncompleteness of our dataset is the ﬁltering of papers based on the venue rank (i.e., A and A*\naccording to the CORE Rank). There are several external factors that inﬂuence the\nacceptance of a paper that are not necessarily related to the quality and relevance of the\nstudy.",
    "section_title": "Threats to validity",
    "section_level": 2,
    "page_number": 28,
    "chunk_index": 2,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Threats to validity",
      "section_level": 2
    }
  },
  {
    "text": "The rationale for applying the exclusion criterion by venue rank is to reduce the\nCândido et al. (2021), PeerJ Comput. Sci., DOI 10.7717/peerj-cs.489\n28/38\n\n\ndataset to a manageable size using a well-deﬁned rule. Overall, it is possible that relevant\nstudies might be missing in our analysis.\nOne way to address this limitation is by analyzing the proceedings of conferences and\njournals on different years to identify missing entries. In our case, we have 46 after the\nselection process. Another approach is by applying backwards/forward snowballing after\nthe selection process. While Google Scholar provides a “cited by” functionality that is\nuseful for that purpose, the process still requires manual steps to query and analyze the\nresults.\nNevertheless, while the aforementioned approaches are useful to avoid missing studies,\nwe argue that the number of papers and venues addressed in our work is a representative\nsample from the research ﬁeld.",
    "section_title": "Threats to validity",
    "section_level": 2,
    "page_number": 28,
    "chunk_index": 3,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Threats to validity",
      "section_level": 2
    }
  },
  {
    "text": "The absence of relevant studies do not undermine our\nconclusions and results since we are not studying any particular dimension of the research\nﬁeld in depth (e.g., whether technique “A” performs better than “B” for parsing).\nFurthermore, we analyze a broad corpus of high-quality studies that cover the life-cycle of\nlog data.\nInternal validity\nThe main threat to the internal validity relates to our classiﬁcation procedure. The ﬁrst\nauthor conducted the ﬁrst step of the characterization procedure. Given that the entire\nprocess was mostly manual, this might introduce a bias on the subsequent analysis. To\nreduce its impact, the ﬁrst author performed the procedure twice. Moreover, the second\nauthor revisited all the decisions made by the ﬁrst author throughout the process.\nAll diversions were discussed and settled throughout the study.\nCONCLUSIONS\nIn this work, we show how researchers have been addressing the different challenges in\nthe life-cycle of log data. Logging provides a rich source of data that can enable several\ntypes of analysis that is beneﬁcial to the operations of complex systems.",
    "section_title": "Threats to validity",
    "section_level": 2,
    "page_number": 28,
    "chunk_index": 4,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Threats to validity",
      "section_level": 2
    }
  },
  {
    "text": "LOG ANALYSIS is a\nmature ﬁeld, and we believe that part of this success is due to the availability of dataset to\nfoster innovation. LOGGING and LOG INFRASTRUCTURE, on the other hand, are still in a early\nstage of development. There are several barriers that hinder innovation in those area,\ne.g., lack of representative data of application logs and access to developers. We believe that\nclosing the gap between academia and industry can increase momentum and enable the\nfuture generation of tools and standards for logging.\nADDITIONAL INFORMATION AND DECLARATIONS\nFunding\nThis work was supported by the Netherlands Organization for Scientiﬁc Research (NWO)\nMIPL project [grant number 628.008.003]. The funders had no role in study design, data\ncollection and analysis, decision to publish, or preparation of the manuscript.\nCândido et al. (2021), PeerJ Comput. Sci., DOI 10.7717/peerj-cs.489\n29/38",
    "section_title": "Threats to validity",
    "section_level": 2,
    "page_number": 28,
    "chunk_index": 5,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Threats to validity",
      "section_level": 2
    }
  },
  {
    "text": "dataset to a manageable size using a well-deﬁned rule. Overall, it is possible that relevant\nstudies might be missing in our analysis.\nOne way to address this limitation is by analyzing the proceedings of conferences and\njournals on different years to identify missing entries. In our case, we have 46 after the\nselection process. Another approach is by applying backwards/forward snowballing after\nthe selection process. While Google Scholar provides a “cited by” functionality that is\nuseful for that purpose, the process still requires manual steps to query and analyze the\nresults.\nNevertheless, while the aforementioned approaches are useful to avoid missing studies,\nwe argue that the number of papers and venues addressed in our work is a representative\nsample from the research ﬁeld. The absence of relevant studies do not undermine our\nconclusions and results since we are not studying any particular dimension of the research\nﬁeld in depth (e.g., whether technique “A” performs better than “B” for parsing).\nFurthermore, we analyze a broad corpus of high-quality studies that cover the life-cycle of\nlog data.",
    "section_title": "Conclusions",
    "section_level": 2,
    "page_number": 29,
    "chunk_index": 0,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Conclusions",
      "section_level": 2
    }
  },
  {
    "text": "Internal validity\nThe main threat to the internal validity relates to our classiﬁcation procedure. The ﬁrst\nauthor conducted the ﬁrst step of the characterization procedure. Given that the entire\nprocess was mostly manual, this might introduce a bias on the subsequent analysis. To\nreduce its impact, the ﬁrst author performed the procedure twice. Moreover, the second\nauthor revisited all the decisions made by the ﬁrst author throughout the process.\nAll diversions were discussed and settled throughout the study.\nCONCLUSIONS\nIn this work, we show how researchers have been addressing the different challenges in\nthe life-cycle of log data. Logging provides a rich source of data that can enable several\ntypes of analysis that is beneﬁcial to the operations of complex systems. LOG ANALYSIS is a\nmature ﬁeld, and we believe that part of this success is due to the availability of dataset to\nfoster innovation. LOGGING and LOG INFRASTRUCTURE, on the other hand, are still in a early\nstage of development.",
    "section_title": "Conclusions",
    "section_level": 2,
    "page_number": 29,
    "chunk_index": 1,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Conclusions",
      "section_level": 2
    }
  },
  {
    "text": "There are several barriers that hinder innovation in those area,\ne.g., lack of representative data of application logs and access to developers. We believe that\nclosing the gap between academia and industry can increase momentum and enable the\nfuture generation of tools and standards for logging.\nADDITIONAL INFORMATION AND DECLARATIONS\nFunding\nThis work was supported by the Netherlands Organization for Scientiﬁc Research (NWO)\nMIPL project [grant number 628.008.003]. The funders had no role in study design, data\ncollection and analysis, decision to publish, or preparation of the manuscript.\nCândido et al. (2021), PeerJ Comput. Sci., DOI 10.7717/peerj-cs.489\n29/38\n\n\nGrant Disclosures\nThe following grant information was disclosed by the authors:\nNetherlands Organization for Scientiﬁc Research (NWO) MIPL: 628.008.003.\nCompeting Interests\nArie van Deursen is an Academic Editor for PeerJ Computer Science.\nJeanderson Barros Cândido is a Ph.D. student at TU Delft and is conducting his\nresearch at Adyen N.V., the industry partner of his Ph.D. program.",
    "section_title": "Conclusions",
    "section_level": 2,
    "page_number": 29,
    "chunk_index": 2,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Conclusions",
      "section_level": 2
    }
  },
  {
    "text": "Author Contributions\n\u0001 Jeanderson Cândido conceived and designed the experiments, performed the\nexperiments, analyzed the data, performed the computation work, prepared ﬁgures and/\nor tables, authored or reviewed drafts of the paper, and approved the ﬁnal draft.\n\u0001 Maurício Aniche conceived and designed the experiments, performed the experiments,\nanalyzed the data, authored or reviewed drafts of the paper, and approved the ﬁnal draft.\n\u0001 Arie van Deursen analyzed the data, authored or reviewed drafts of the paper, and\napproved the ﬁnal draft.\nData Availability\nThe following information was supplied regarding data availability:\nThe raw data is available in the Supplemental File.\nSupplemental Information\nSupplemental information for this article can be found online at http://dx.doi.org/10.7717/\npeerj-cs.489#supplemental-information.\nREFERENCES\nAbad C, Taylor J, Sengul C, Yurcik W, Zhou Y, Rowe K. 2003. Log correlation for intrusion\ndetection: a proof of concept. In: Proceedings of the 19th Annual Computer Security Applications\nConference, 2003, Las Vegas, Nevada, USA. Piscataway: IEEE, 255–264.",
    "section_title": "Conclusions",
    "section_level": 2,
    "page_number": 29,
    "chunk_index": 3,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Conclusions",
      "section_level": 2
    }
  },
  {
    "text": "Agrawal A, Karlupia R, Gupta R. 2019. Logan: a distributed online log parser. In: 2019 IEEE 35th\nInternational Conference on Data Engineering (ICDE). Piscataway: IEEE, 1946–1951.\nAharon M, Barash G, Cohen I, Mordechai E. 2009. One graph is worth a thousand logs:\nuncovering hidden structures in massive system event logs. In: Buntine W, Grobelnik M,\nMladenić D, Shawe-Taylor J, eds. Machine Learning and Knowledge Discovery in Databases.\nBerlin, Heidelberg: Springer, 227–243.\nAharoni E, Fine S, Goldschmidt Y, Lavi O, Margalit O, Rosen-Zvi M, Shpigelman L. 2011.\nSmarter log analysis. IBM Journal of Research and Development 55(5):10:1–10:10\nDOI 10.1147/JRD.2011.2165675.\nAndrews JH. 1998. Testing using log ﬁle analysis: tools, methods, and issues. In: Proceedings 13th\nIEEE International Conference on Automated Software Engineering (Cat. No.98EX239). 157–166.\nAndrews JH, Zhang Y. 2000. Broad-spectrum studies of log ﬁle analysis. In: Proceedings of the\n22nd International Conference on Software Engineering - ICSE ’00, Limerick, Ireland. New York:\nACM Press, 105–114.\nCândido et al. (2021), PeerJ Comput.",
    "section_title": "Conclusions",
    "section_level": 2,
    "page_number": 29,
    "chunk_index": 4,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "Conclusions",
      "section_level": 2
    }
  },
  {
    "text": "Grant Disclosures\nThe following grant information was disclosed by the authors:\nNetherlands Organization for Scientiﬁc Research (NWO) MIPL: 628.008.003.\nCompeting Interests\nArie van Deursen is an Academic Editor for PeerJ Computer Science.\nJeanderson Barros Cândido is a Ph.D. student at TU Delft and is conducting his\nresearch at Adyen N.V., the industry partner of his Ph.D. program.\nAuthor Contributions\n\u0001 Jeanderson Cândido conceived and designed the experiments, performed the\nexperiments, analyzed the data, performed the computation work, prepared ﬁgures and/\nor tables, authored or reviewed drafts of the paper, and approved the ﬁnal draft.\n\u0001 Maurício Aniche conceived and designed the experiments, performed the experiments,\nanalyzed the data, authored or reviewed drafts of the paper, and approved the ﬁnal draft.\n\u0001 Arie van Deursen analyzed the data, authored or reviewed drafts of the paper, and\napproved the ﬁnal draft.\nData Availability\nThe following information was supplied regarding data availability:\nThe raw data is available in the Supplemental File.",
    "section_title": "References",
    "section_level": 2,
    "page_number": 30,
    "chunk_index": 0,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "References",
      "section_level": 2
    }
  },
  {
    "text": "Supplemental Information\nSupplemental information for this article can be found online at http://dx.doi.org/10.7717/\npeerj-cs.489#supplemental-information.\nREFERENCES\nAbad C, Taylor J, Sengul C, Yurcik W, Zhou Y, Rowe K. 2003. Log correlation for intrusion\ndetection: a proof of concept. In: Proceedings of the 19th Annual Computer Security Applications\nConference, 2003, Las Vegas, Nevada, USA. Piscataway: IEEE, 255–264.\nAgrawal A, Karlupia R, Gupta R. 2019. Logan: a distributed online log parser. In: 2019 IEEE 35th\nInternational Conference on Data Engineering (ICDE). Piscataway: IEEE, 1946–1951.\nAharon M, Barash G, Cohen I, Mordechai E. 2009. One graph is worth a thousand logs:\nuncovering hidden structures in massive system event logs. In: Buntine W, Grobelnik M,\nMladenić D, Shawe-Taylor J, eds. Machine Learning and Knowledge Discovery in Databases.\nBerlin, Heidelberg: Springer, 227–243.\nAharoni E, Fine S, Goldschmidt Y, Lavi O, Margalit O, Rosen-Zvi M, Shpigelman L. 2011.\nSmarter log analysis. IBM Journal of Research and Development 55(5):10:1–10:10\nDOI 10.1147/JRD.2011.2165675.\nAndrews JH. 1998.",
    "section_title": "References",
    "section_level": 2,
    "page_number": 30,
    "chunk_index": 1,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "References",
      "section_level": 2
    }
  },
  {
    "text": "Testing using log ﬁle analysis: tools, methods, and issues. In: Proceedings 13th\nIEEE International Conference on Automated Software Engineering (Cat. No.98EX239). 157–166.\nAndrews JH, Zhang Y. 2000. Broad-spectrum studies of log ﬁle analysis. In: Proceedings of the\n22nd International Conference on Software Engineering - ICSE ’00, Limerick, Ireland. New York:\nACM Press, 105–114.\nCândido et al. (2021), PeerJ Comput. Sci., DOI 10.7717/peerj-cs.489\n30/38\n\n\nAndrews JH, Zhang Y. 2003. General test result checking with log ﬁle analysis. IEEE Transactions\non Software Engineering 29(7):634–648 DOI 10.1109/TSE.2003.1214327.\nAnu H, Chen J, Shi W, Hou J, Liang B, Qin B. 2019. An approach to recommendation of\nverbosity log levels based on logging intention. In: 2019 IEEE International Conference on\nSoftware Maintenance and Evolution (ICSME). Piscataway: IEEE, 125–134.\nAwad M, Menasce DA. 2016. Performance model derivation of operational systems through log\nanalysis.",
    "section_title": "References",
    "section_level": 2,
    "page_number": 30,
    "chunk_index": 2,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "References",
      "section_level": 2
    }
  },
  {
    "text": "In: 2016 IEEE 24th International Symposium on Modeling, Analysis and Simulation of\nComputer and Telecommunication Systems (MASCOTS), London, United Kingdom. Piscataway:\nIEEE, 159–168.\nBalliu A, Olivetti D, Babaoglu O, Marzolla M, Sȋrbu A. 2015. A big data analyzer for large trace\nlogs. Computing 98:1225–1249.\nBanerjee S, Srikanth H, Cukic B. 2010. Log-based reliability analysis of software as a service\n(SaaS). In: 2010 IEEE 21st International Symposium on Software Reliability Engineering, San Jose,\nCA, USA. Piscataway: IEEE, 239–248.\nBao L, Li Q, Lu P, Lu J, Ruan T, Zhang K. 2018. Execution anomaly detection in large-scale\nsystems through console log analysis. Journal of Systems and Software 143(1):172–186\nDOI 10.1016/j.jss.2018.05.016.\nBarrett B. 2019. The catch-22 that broke the internet. Wired Magazine. Available at\nhttps://www.wired.com/story/google-cloud-outage-catch-22/#:~:text=12%3A26%20PM-,The%\n20Catch%2D22%20That%20Broke%20the%20Internet,Google%20needed%20to%20ﬁx%20it.\n&text=Five%20days%20ago%2C%20the%20internet,around%20the%20globe%2C%20YouTube\n%20sputtered.\nBarse EL, Jonsson E. 2004.",
    "section_title": "References",
    "section_level": 2,
    "page_number": 30,
    "chunk_index": 3,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "References",
      "section_level": 2
    }
  },
  {
    "text": "Extracting attack manifestations to determine log data requirements for\nintrusion detection. In: 20th Annual Computer Security Applications Conference, Tucson, AZ,\nUSA. Piscataway: IEEE, 158–167.\nBass L, Weber I, Zhu L. 2015. DevOps: a software architect’s perspective. Boston: Addison-Wesley\nProfessional.\nBertero C, Roy M, Sauvanaud C, Tredan G. 2017. Experience report: log mining using natural\nlanguage processing and application to anomaly detection. In: 2017 IEEE 28th International\nSymposium on Software Reliability Engineering (ISSRE), Toulouse. Piscataway: IEEE, 351–360.\nBeschastnikh I, Brun Y, Ernst MD, Krishnamurthy A. 2014. Inferring models of concurrent\nsystems from logs of their behavior with CSight. In: Proceedings of the 36th International\nConference on Software Engineering, ICSE 2014, Hyderabad, India. New York: ACM, 468–479.\nButin D, Le Métayer D. 2014. Log analysis for data protection accountability. In: Jones C,\nPihlajasaari P, Sun J, eds. FM 2014: Formal Methods. Cham: Springer International Publishing,\n163–178.\nChen B, Jiang ZMJ. 2017a.",
    "section_title": "References",
    "section_level": 2,
    "page_number": 30,
    "chunk_index": 4,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "References",
      "section_level": 2
    }
  },
  {
    "text": "Characterizing and detecting anti-patterns in the logging code. In:\nProceedings of the 39th International Conference on Software Engineering, ICSE ’17, Piscataway,\nNJ, USA. Piscataway: IEEE Press, 71–81.\nChen B, Jiang ZMJ. 2017b. Characterizing logging practices in Java-based open source software\nprojects—a replication study in Apache Software Foundation. Empirical Software Engineering\n22(1):330–374 DOI 10.1007/s10664-016-9429-5.\nChen B, Song J, Xu P, Hu X, Jiang ZMJ. 2018. An automated approach to estimating code\ncoverage measures via execution logs. In: Proceedings of the 33rd ACM/IEEE International\nConference on Automated Software Engineering, ASE 2018, Montpellier, France. New York, NY,\nUSA: ACM, 305–316.\nCândido et al. (2021), PeerJ Comput. Sci., DOI 10.7717/peerj-cs.489\n31/38\n\n\nChu J, Ge Z, Huber R, Ji P, Yates J, Yu Y-C. 2012. ALERT-ID: analyze logs of the network element\nin real time for intrusion detection. In: Balzarotti D, Stolfo SJ, Cova M, eds. Research in Attacks,\nIntrusions, and Defenses. Berlin, Heidelberg: Springer, 294–313.",
    "section_title": "References",
    "section_level": 2,
    "page_number": 30,
    "chunk_index": 5,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "References",
      "section_level": 2
    }
  },
  {
    "text": "Chuah E, Jhumka A, Narasimhamurthy S, Hammond J, Browne JC, Barth B. 2013. Linking\nresource usage anomalies with system failures from cluster log data. In: 2013 IEEE 32nd\nInternational Symposium on Reliable Distributed Systems, Braga, Portugal. Piscataway: IEEE,\n111–120.\nChung F. 2018. Coles stores reopen after ‘minor’ IT glitch causes nationwide register outage.\nAvailable at https://www.news.com.au/ﬁnance/business/retail/coles-stores-closed-due-to-\nnationwide-register-outage/news-story/4e6558c5e439a8eaaab08e03ba316283.\nCinque M, Cotroneo D, Natella R, Pecchia A. 2010. Assessing and improving the effectiveness of\nlogs for the analysis of software faults. In: 2010 IEEE/IFIP International Conference on\nDependable Systems & Networks (DSN), Chicago, IL, USA. Piscataway: IEEE, 457–466.\nCinque M, Cotroneo D, Pecchia A. 2013. Event logs for the analysis of software failures:\na rule-based approach. IEEE Transactions on Software Engineering 39(6):806–821\nDOI 10.1109/TSE.2012.67.\nda Cruz SM, Campos ML, Pires PF, Campos LM. 2004. Monitoring e-business web services usage\nthrough a log based architecture.",
    "section_title": "References",
    "section_level": 2,
    "page_number": 30,
    "chunk_index": 6,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "References",
      "section_level": 2
    }
  },
  {
    "text": "In: Proceedings. IEEE International Conference on Web\nServices, San Diego, CA, USA. Piscataway: IEEE, 61–69.\nDebnath B, Solaimani M, Gulzar MAG, Arora N, Lumezanu C, Xu J, Zong B, Zhang H, Jiang G,\nKhan L. 2018. LogLens: a real-time log analysis system. In: 2018 IEEE 38th International\nConference on Distributed Computing Systems (ICDCS), Vienna. Piscataway: IEEE, 1052–1062.\nDi S, Gupta R, Snir M, Pershey E, Cappello F. 2017. LogAider: a tool for mining potential\ncorrelations of HPC log events. In: Proceedings of the 17th IEEE/ACM International Symposium\non Cluster, Cloud and Grid Computing, CCGrid ’17, Madrid, Spain. Piscataway: IEEE Press,\n442–451.\nDi Martino C, Cinque M, Cotroneo D. 2012. Assessing time coalescence techniques for the\nanalysis of supercomputer logs. In: IEEE/IFIP International Conference on Dependable Systems\nand Networks (DSN 2012), Boston, MA, USA. Piscataway: IEEE, 1–12.\nDu M, Li F, Zheng G, Srikumar V. 2017. DeepLog: anomaly detection and diagnosis from system\nlogs through deep learning.",
    "section_title": "References",
    "section_level": 2,
    "page_number": 30,
    "chunk_index": 7,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "References",
      "section_level": 2
    }
  },
  {
    "text": "In: Proceedings of the 2017 ACM SIGSAC Conference on Computer\nand Communications Security, CCS ’17, Dallas, Texas, USA. New York: ACM, 1285–1298.\nDyck A, Penners R, Lichter H. 2015. Towards deﬁnitions for release engineering and devops. In:\n3rd International Workshop onRelease Engineering (RELENG), 2015 IEEE/ACM, IEEE, 3.\nEl-Masri D, Petrillo F, Guéhéneuc Y-G, Hamou-Lhadj A, Bouziane A. 2020. A systematic\nliterature review on automated log abstraction techniques. Information and Software Technology\n122:106276.\nEl-Sayed N, Schroeder B. 2013. Reading between the lines of failure logs: understanding how HPC\nsystems fail. In: 2013 43rd Annual IEEE/IFIP International Conference on Dependable Systems\nand Networks (DSN), Budapest, Hungary. Piscataway: IEEE, 1–12.\nFarshchi M, Schneider J-G, Weber I, Grundy J. 2015. Experience report: anomaly detection of\ncloud application operations using log and cloud metric correlation analysis. In: 2015 IEEE 26th\nInternational Symposium on Software Reliability Engineering (ISSRE), Gaithersbury, MD, USA.\nPiscataway: IEEE, 24–34.\nFarshchi M, Schneider J-G, Weber I, Grundy J.",
    "section_title": "References",
    "section_level": 2,
    "page_number": 30,
    "chunk_index": 8,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "References",
      "section_level": 2
    }
  },
  {
    "text": "2018. Metric selection and anomaly detection for\ncloud operations using log and metric correlation analysis. Journal of Systems and Software\n137(9):531–549 DOI 10.1016/j.jss.2017.03.012.\nCândido et al. (2021), PeerJ Comput. Sci., DOI 10.7717/peerj-cs.489\n32/38\n\n\nFu Q, Lou J-G, Wang Y, Li J. 2009. Execution anomaly detection in distributed systems through\nunstructured log analysis. In: 2009 Ninth IEEE International Conference on Data Mining, Miami\nBeach, FL, USA. Piscataway: IEEE, 149–158.\nFu Q, Zhu J, Hu W, Lou J-G, Ding R, Lin Q, Zhang D, Xie T. 2014a. Where do developers log?\nAn empirical study on logging practices in industry. In: Companion Proceedings of the 36th\nInternational Conference on Software Engineering, ICSE Companion 2014, Hyderabad, India.\nNew York: ACM, 24–33.\nFu X, Ren R, McKee SA, Zhan J, Sun N. 2014b. Digging deeper into cluster system logs for failure\nprediction and root cause diagnosis. In: 2014 IEEE International Conference on Cluster\nComputing (CLUSTER), Madrid, Spain. Piscataway: IEEE, 103–112.\nFu X, Ren R, Zhan J, Zhou W, Jia Z, Lu G. 2012.",
    "section_title": "References",
    "section_level": 2,
    "page_number": 30,
    "chunk_index": 9,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "References",
      "section_level": 2
    }
  },
  {
    "text": "LogMaster: mining event correlations in logs of\nlarge-scale cluster systems. In: 2012 IEEE 31st Symposium on Reliable Distributed Systems, Irvine,\nCA, USA. IEEE, 71–80.\nGainaru A, Cappello F, Trausan-Matu S, Kramer B. 2011. Event log mining tool for large scale\nHPC systems. In: Jeannot E, Namyst R, Roman J, eds. Euro-Par 2011 Parallel Processing. Berlin,\nHeidelberg: Springer, 52–64.\nGao Y, Zhou W, Zhang Z, Han J, Meng D, Xu Z. 2014. Online anomaly detection by improved\ngrammar compression of log sequences. In: Zaki MJ, Obradovic Z, Tan P-N, Banerjee A,\nKamath C, Parthasarathy S, eds. Proceedings of the 2014 SIAM International Conference on Data\nMining, Philadelphia, Pennsylvania, USA, April 24-26, 2014. SIAM, 911–919.\nGartner. 2019. How to get started with AIOps. Available at https://www.gartner.com/\nsmarterwithgartner/how-to-get-started-with-aiops/.\nGhanbari S, Hashemi AB, Amza C. 2014. Stage-aware anomaly detection through tracking log\npoints. In: Proceedings of the 15th International Middleware Conference, Middleware ’14,\nBordeaux, France. New York: ACM, 253–264.\nGoncalves D, Bota J, Correia M.",
    "section_title": "References",
    "section_level": 2,
    "page_number": 30,
    "chunk_index": 10,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "References",
      "section_level": 2
    }
  },
  {
    "text": "2015. Big data analytics for detecting host misbehavior in large\nlogs. In: 2015 IEEE Trustcom/BigDataSE/ISPA, Helsinki, Finland. Piscataway: IEEE, 238–245.\nGoogle. 2019. Google cloud networking incident #19009. Available at https://status.cloud.google.\ncom/incident/cloud-networking/19009.\nGunter D, Tierney BL, Brown A, Swany M, Bresnahan J, Schopf JM. 2007. Log summarization\nand anomaly detection for troubleshooting distributed systems. In: 2007 8th IEEE/ACM\nInternational Conference on Grid Computing, Austin, TX, USA. Piscataway: IEEE, 226–234.\nGurumdimma N, Jhumka A, Liakata M, Chuah E, Browne J. 2016. CRUDE: combining resource\nusage data and error logs for accurate error detection in large-scale distributed systems. In: 2016\nIEEE 35th Symposium on Reliable Distributed Systems (SRDS), Budapest, Hungary. Piscataway:\nIEEE, 51–60.\nHamooni H, Debnath B, Xu J, Zhang H, Jiang G, Mueen A. 2016. LogMine: fast pattern\nrecognition for log analytics. In: Proceedings of the 25th ACM International on Conference on\nInformation and Knowledge Management, CIKM ’16, Indianapolis, Indiana, USA.",
    "section_title": "References",
    "section_level": 2,
    "page_number": 30,
    "chunk_index": 11,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "References",
      "section_level": 2
    }
  },
  {
    "text": "New York:\nACM, 1573–1582.\nHansen JP, Siewiorek DP. 1992. Models for time coalescence in event logs. In: Digest of Papers.\nFTCS-22: The Twenty-Second International Symposium on Fault-Tolerant Computing, Boston,\nMA, USA. Piscataway: IEEE, 221–227.\nHassani M, Shang W, Shihab E, Tsantalis N. 2018. Studying and detecting log-related issues.\nEmpirical Software Engineering 23(6):3248–3280 DOI 10.1007/s10664-018-9603-z.\nCândido et al. (2021), PeerJ Comput. Sci., DOI 10.7717/peerj-cs.489\n33/38\n\n\nHe P, Chen Z, He S, Lyu MR. 2018a. Characterizing the natural language descriptions in software\nlogging statements. In: Proceedings of the 33rd ACM/IEEE International Conference on\nAutomated Software Engineering, ASE 2018, New York, NY, USA. Piscataway: IEEE, 178–189.\nHe P, Zhu J, He S, Li J, Lyu MR. 2016a. An evaluation study on log parsing and its use in log\nmining. In: 2016 46th Annual IEEE/IFIP International Conference on Dependable Systems and\nNetworks (DSN), Toulouse, France. Piscataway: IEEE, 654–661.\nHe P, Zhu J, He S, Li J, Lyu MR. 2018b. Towards automated log parsing for large-scale log data\nanalysis.",
    "section_title": "References",
    "section_level": 2,
    "page_number": 30,
    "chunk_index": 12,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "References",
      "section_level": 2
    }
  },
  {
    "text": "IEEE Transactions on Dependable and Secure Computing 15(6):931–944\nDOI 10.1109/TDSC.2017.2762673.\nHe S, Zhu J, He P, Lyu MR. 2016b. Experience report: system log analysis for anomaly detection.\nIn: 2016 IEEE 27th International Symposium on Software Reliability Engineering (ISSRE),\nOttawa, ON, Canada. Piscataway: IEEE, 207–218.\nHuynh T, Miller J. 2009. Another viewpoint on evaluating web software reliability based on\nworkload and failure data extracted from server logs. Empirical Software Engineering\n14(4):371–396 DOI 10.1007/s10664-008-9084-6.\nInvestor’s Business Daily. 2018. Elastic IPO prices above range, stock nearly doubles. Available at\nhttps://www.investors.com/news/technology/elastic-ipo-initial-public-offering/.\nJuvonen A, Sipola T, Hämäläinen T. 2015. Online anomaly detection using dimensionality\nreduction techniques for HTTP log analysis. Computer Networks 91(1–2):46–56\nDOI 10.1016/j.comnet.2015.07.019.\nKabinna S, Bezemer C-P, Shang W, Hassan AE. 2016. Logging library migrations: a case study for\nthe apache software foundation projects.",
    "section_title": "References",
    "section_level": 2,
    "page_number": 30,
    "chunk_index": 13,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "References",
      "section_level": 2
    }
  },
  {
    "text": "In: Proceedings of the 13th International Conference on\nMining Software Repositories, MSR ’16, Austin, Texas. New York, NY, USA: ACM, 154–164.\nKc K, Gu X. 2011. ELT: efﬁcient log-based troubleshooting system for cloud computing\ninfrastructures. In: 2011 IEEE 30th International Symposium on Reliable Distributed Systems,\nMadrid, Spain. Piscataway: IEEE, 11–20.\nKhatuya S, Ganguly N, Basak J, Bharde M, Mitra B. 2018. ADELE: anomaly detection from event\nlog empiricism. In: IEEE INFOCOM, 2018 - IEEE Conference on Computer Communications,\nHonolulu, HI. Piscataway: IEEE, 2114–2122.\nKimura T, Ishibashi K, Mori T, Sawada H, Toyono T, Nishimatsu K, Watanabe A, Shimoda A,\nShiomoto K. 2014. Spatio-temporal factorization of log data for understanding network events.\nIn: IEEE INFOCOM, 2014 - IEEE Conference on Computer Communications. 610–618.\nKitchenham B, Charters S. 2007. Guidelines for performing systematic literature reviews\nin software engineering. Technical report, Technical report, Ver. 2.3 EBSE Technical Report.\nEBSE. Available at https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.117.",
    "section_title": "References",
    "section_level": 2,
    "page_number": 30,
    "chunk_index": 14,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "References",
      "section_level": 2
    }
  },
  {
    "text": "471&rep=rep1&type=pdf.\nLi H, Chen T-HP, Shang W, Hassan AE. 2018. Studying software logging using topic models.\nEmpirical Software Engineering 23(5):2655–2694 DOI 10.1007/s10664-018-9595-8.\nLi H, Shang W, Hassan AE. 2017. Which log level should developers choose for a new logging\nstatement? Empirical Software Engineering 22(4):1684–1716 DOI 10.1007/s10664-016-9456-2.\nLi S, Niu X, Jia Z, Liao X, Wang J, Li T. 2019a. Guiding log revisions by learning from software\nevolution history. In: Empirical Software Engineering, Berlin: Springer, 1–39.\nLi T, Jiang Y, Zeng C, Xia B, Liu Z, Zhou W, Zhu X, Wang W, Zhang L, Wu J, Xue L, Bao D.\n2017. FLAP: an end-to-end event log analysis platform for system management. In: Proceedings\nof the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,\nKDD ’17, Halifax, NS, Canada. New York: ACM, 1547–1556.\nCândido et al. (2021), PeerJ Comput. Sci., DOI 10.7717/peerj-cs.489\n34/38\n\n\nLi Z, Chen T-H, Yang J, Shang W. 2019b. DLFinder: characterizing and detecting duplicate\nlogging code smells.",
    "section_title": "References",
    "section_level": 2,
    "page_number": 30,
    "chunk_index": 15,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "References",
      "section_level": 2
    }
  },
  {
    "text": "In: 2019 IEEE/ACM 41st International Conference on Software Engineering\n(ICSE). Piscataway: IEEE, 152–163.\nLiang Y, Zhang Y, Xiong H, Sahoo R. 2007. An adaptive semantic ﬁlter for blue gene/L failure\nlog analysis. In: 2007 IEEE International Parallel and Distributed Processing Symposium,\nLong Beach, CA, USA. Piscataway: IEEE, 1–8.\nLim C, Singh N, Yajnik S. 2008. A log mining approach to failure analysis of enterprise telephony\nsystems. In: 2008 IEEE International Conference on Dependable Systems and Networks With\nFTCS and DCC (DSN), Anchorage, AK. Piscataway: IEEE, 398–403.\nLin H, Zhou J, Yao B, Guo M, Li J. 2015. Cowic: a column-wise independent compression for log\nstream analysis. In: 2015 15th IEEE/ACM International Symposium on Cluster, Cloud and Grid\nComputing, Shenzhen, China. Piscataway: IEEE, 21–30.\nLin Q, Zhang H, Lou J-G, Zhang Y, Chen X. 2016. Log clustering based problem identiﬁcation for\nonline service systems. In: Proceedings of the 38th International Conference on Software\nEngineering Companion, ICSE ’16, Austin, Texas. New York: ACM, 102–111.\nLiu J, Zhu J, He S, He P, Zheng Z, Lyu MR.",
    "section_title": "References",
    "section_level": 2,
    "page_number": 30,
    "chunk_index": 16,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "References",
      "section_level": 2
    }
  },
  {
    "text": "2019a. Logzip: extracting hidden structures via\niterative clustering for log compression. In: 2019 34th IEEE/ACM International Conference on\nAutomated Software Engineering (ASE). Piscataway: IEEE, 863–873.\nLiu Z, Xia X, Lo D, Xing Z, Hassan AE, Li S. 2019b. Which variables should i log? In: IEEE\nTransactions on Software Engineering. Piscataway: IEEE.\nLou J-G, Fu Q, Yang S, Xu Y, Li J. 2010. Mining invariants from console logs for system problem\ndetection. In: USENIX Annual Technical Conference. 1–14.\nLu S, Rao B, Wei X, Tak B, Wang L, Wang L. 2017. Log-based abnormal task detection and root\ncause analysis for spark. In: 2017 IEEE International Conference on Web Services (ICWS),\nHonolulu, HI, USA. Piscataway: IEEE, 389–396.\nMakanju A, Zincir-Heywood AN, Milios EE. 2012. A lightweight algorithm for message type\nextraction in system application logs. IEEE Transactions on Knowledge and Data Engineering\n24(11):1921–1936 DOI 10.1109/TKDE.2011.138.\nMakanju AA, Zincir-Heywood AN, Milios EE. 2009. Clustering event logs using iterative\npartitioning.",
    "section_title": "References",
    "section_level": 2,
    "page_number": 30,
    "chunk_index": 17,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "References",
      "section_level": 2
    }
  },
  {
    "text": "In: Proceedings of the 15th ACM SIGKDD International Conference on Knowledge\nDiscovery and Data Mining, KDD ’09, Paris, France. New York: ACM, 1255–1264.\nMariani L, Pastore F. 2008. Automated identiﬁcation of failure causes in system logs. In: 2008 19th\nInternational Symposium on Software Reliability Engineering (ISSRE), Seattle, WA, USA.\nPiscataway: IEEE, 117–126.\nMavridis I, Karatza H. 2017. Performance evaluation of cloud-based log ﬁle analysis with Apache\nHadoop and Apache Spark. Journal of Systems and Software 125(10):133–151\nDOI 10.1016/j.jss.2016.11.037.\nMeng W, Liu Y, Zhu Y, Zhang S, Pei D, Liu Y, Chen Y, Zhang R, Tao S, Sun P, Zhou R. 2019.\nLogAnomaly: unsupervised detection of sequential and quantitative anomalies in unstructured\nlogs. In: IJCAI. 4739–4745.\nNandi A, Mandal A, Atreja S, Dasgupta GB, Bhattacharya S. 2016. Anomaly detection using\nprogram control ﬂow graph mining from execution logs. In: Proceedings of the 22Nd ACM\nSIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’16,\nSan Francisco, California, USA. New York: ACM, 215–224.",
    "section_title": "References",
    "section_level": 2,
    "page_number": 30,
    "chunk_index": 18,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "References",
      "section_level": 2
    }
  },
  {
    "text": "Neves F, Machado N, Pereira J. 2018. Falcon: a practical log-based analysis tool for distributed\nsystems. In: 2018 48th Annual IEEE/IFIP International Conference on Dependable Systems and\nNetworks (DSN), Luxembourg City. Piscataway: IEEE, 534–541.\nCândido et al. (2021), PeerJ Comput. Sci., DOI 10.7717/peerj-cs.489\n35/38\n\n\nOliner A, Stearley J. 2007. What supercomputers say: a study of ﬁve system logs. In: 37th Annual\nIEEE/IFIP International Conference on Dependable Systems and Networks (DSN’07). Piscataway:\nIEEE, 575–584.\nOprea A, Li Z, Yen T-F, Chin SH, Alrwais S. 2015. Detection of early-stage enterprise infection by\nmining large-scale log data. In: 2015 45th Annual IEEE/IFIP International Conference on\nDependable Systems and Networks, Rio de Janeiro, Brazil. Piscataway: IEEE, 45–56.\nPark BH, Hukerikar S, Adamson R, Engelmann C. 2017. Big data meets HPC log analytics:\nscalable approach to understanding systems at extreme scale. In: 2017 IEEE International\nConference on Cluster Computing (CLUSTER), Honolulu, HI, USA. Piscataway: IEEE, 758–765.\nPecchia A, Cinque M, Carrozza G, Cotroneo D. 2015.",
    "section_title": "References",
    "section_level": 2,
    "page_number": 30,
    "chunk_index": 19,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "References",
      "section_level": 2
    }
  },
  {
    "text": "Industry practices and event logging:\nassessment of a critical software development process. In: Proceedings of the 37th International\nConference on Software Engineering - Volume 2, ICSE ’15, Florence, Italy. Piscataway: IEEE Press,\n169–178.\nPecchia A, Russo S. 2012. Detection of software failures through event logs: an experimental study.\nIn: 2012 IEEE 23rd International Symposium on Software Reliability Engineering, Dallas, TX,\nUSA. Piscataway: IEEE, 31–40.\nPetersen K, Feldt R, Mujtaba S, Mattsson M. 2008. Systematic mapping studies in software\nengineering. In: 12th International Conference on Evaluation and Assessment in Software\nEngineering (EASE). 1–10.\nPi A, Chen W, Zhou X, Ji M. 2018. Proﬁling distributed systems in lightweight virtualized\nenvironments with logs and resource metrics. In: Proceedings of the 27th International\nSymposium on High-Performance Parallel and Distributed Computing, HPDC ’18, Tempe,\nArizona. New York: ACM, 168–179.\nPrewett JE. 2005. Incorporating information from a cluster batch scheduler and center\nmanagement software into automated log ﬁle analysis. In: CCGrid 2005.",
    "section_title": "References",
    "section_level": 2,
    "page_number": 30,
    "chunk_index": 20,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "References",
      "section_level": 2
    }
  },
  {
    "text": "IEEE International\nSymposium on Cluster Computing and the Grid, Cardiff, Wales, UK. Vol. 1. Piscataway: IEEE,\n133–139.\nRamakrishna V, Rajput N, Mukherjea S, Dey K. 2017. A platform for end-to-end mobile\napplication infrastructure analytics using system log correlation. IBM Journal of Research and\nDevelopment 61(1):2:17–2:26 DOI 10.1147/JRD.2016.2626862.\nRen Z, Liu C, Xiao X, Jiang H, Xie T. 2019. Root cause localization for unreproducible builds via\ncausality analysis over system call tracing. In: 2019 34th IEEE/ACM International Conference on\nAutomated Software Engineering (ASE), IEEE, 527–538.\nRoche J. 2013. Adopting DevOps practices in quality assurance. Communications of the ACM\n56:38–43.\nRong G, Zhang Q, Liu X, Gu S. 2017. A systematic review of logging practice in software\nengineering. In: 2017 24th Asia-Paciﬁc Software Engineering Conference (APSEC), Nanjing.\nPiscataway: IEEE, 534–539.\nRusso B, Succi G, Pedrycz W. 2015. Mining system logs to learn error predictors: a case study of a\ntelemetry system. Empirical Software Engineering 20(4):879–927\nDOI 10.1007/s10664-014-9303-2.",
    "section_title": "References",
    "section_level": 2,
    "page_number": 30,
    "chunk_index": 21,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "References",
      "section_level": 2
    }
  },
  {
    "text": "Shalan A, Zulkernine M. 2013. Runtime prediction of failure modes from system error logs. In:\n2013 18th International Conference on Engineering of Complex Computer Systems, Singapore,\nSingapore. Piscataway: IEEE, 232–241.\nShang W, Nagappan M, Hassan AE. 2015. Studying the relationship between logging\ncharacteristics and the code quality of platform software. Empirical Software Engineering\n20(1):1–27 DOI 10.1007/s10664-013-9274-8.\nCândido et al. (2021), PeerJ Comput. Sci., DOI 10.7717/peerj-cs.489\n36/38\n\n\nShang W, Nagappan M, Hassan AE, Jiang ZM. 2014. Understanding log lines using development\nknowledge. In: 2014 IEEE International Conference on Software Maintenance and Evolution.\nPiscataway: IEEE, 21–30.\nSpencer D, Warfel T. 2004. Card sorting: a deﬁnitive guide. Boxes and Arrows, 2. Available at\nhttps://boxesandarrows.com/card-sorting-a-deﬁnitive-guide/.\nSteinle M, Aberer K, Girdzijauskas S, Lovis C. 2006. Mapping moving landscapes by mining\nmountains of logs: novel techniques for dependency model generation. In: Proceedings of the\n32Nd International Conference on Very Large Data Bases, VLDB ’06.",
    "section_title": "References",
    "section_level": 2,
    "page_number": 30,
    "chunk_index": 22,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "References",
      "section_level": 2
    }
  },
  {
    "text": "Seoul, Korea. 1093–1102.\nTan J, Kavulya S, Gandhi R, Narasimhan P. 2010. Visual, log-based causal tracing for\nperformance debugging of mapreduce systems. In: 2010 IEEE 30th International Conference on\nDistributed Computing Systems. Piscataway: IEEE, 795–806.\nTang D, Iyer R. 1992. Analysis of the VAX/VMS error logs in multicomputer environments-a case\nstudy of software dependability. In: Proceedings Third International Symposium on Software\nReliability Engineering, Research Triangle Park, NC, USA. Piscataway: IEEE Comput. Soc. Press,\n216–226.\nTang L, Li T. 2010. LogTree: a framework for generating system events from raw textual logs.\nIn: 2010 IEEE International Conference on Data Mining, Sydney, Australia. Piscataway: IEEE,\n491–500.\nTechCrunch. 2017. Sumo Logic lands $75 million Series F, on path to IPO. Available at\nhttps://techcrunch.com/2017/06/27/sumo-logic-lands-75-million-series-f-on-the-road-to-ipo/.\nTian J, Rudraraju S, Li Z. 2004. Evaluating web software reliability based on workload and failure\ndata extracted from server logs.",
    "section_title": "References",
    "section_level": 2,
    "page_number": 30,
    "chunk_index": 23,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "References",
      "section_level": 2
    }
  },
  {
    "text": "IEEE Transactions on Software Engineering 30(11):754–769\nDOI 10.1109/TSE.2004.87.\nTsao MM, Siewiorek DP. 1983. Trend analysis on system error ﬁles. In: Proceedings 13th\nInternational Symposium on Fault-Tolerant Computing. 116–119.\nUlrich A, Hallal H, Petrenko A, Boroday S. 2003. Verifying trustworthiness requirements in\ndistributed systems with formal log-ﬁle analysis. In: Proceedings of the 36th Annual Hawaii\nInternational Conference on System Sciences, 2003. 10.\nUsability.gov. 2019. Usability.gov: card sorting. Available at https://www.usability.gov/how-to-and-\ntools/methods/card-sorting.html.\nWang J, Li C, Han S, Sarkar S, Zhou X. 2017. Predictive maintenance based on event-log analysis:\na case study. IBM Journal of Research and Development 61(1):11:121–11:132\nDOI 10.1147/JRD.2017.2648298.\nWu F, Anchuri P, Li Z. 2017. Structural event detection from log messages. In: Proceedings of the\n23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD\n’17, Halifax, NS, Canada. New York, NY, USA: ACM, 1175–1184.\nXu W, Huang L, Fox A, Patterson D, Jordan M. 2009a.",
    "section_title": "References",
    "section_level": 2,
    "page_number": 30,
    "chunk_index": 24,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "References",
      "section_level": 2
    }
  },
  {
    "text": "Online system problem detection by\nmining patterns of console logs. In: 2009 Ninth IEEE International Conference on Data Mining,\nMiami Beach, FL, USA. Piscataway: IEEE, 588–597.\nXu W, Huang L, Fox A, Patterson D, Jordan MI. 2009b. Detecting large-scale system problems by\nmining console logs. In: Proceedings of the ACM SIGOPS 22Nd Symposium on Operating Systems\nPrinciples, SOSP ’09, Big Sky, Montana, USA. New York: ACM, 117–132.\nYen T-F, Oprea A, Onarlioglu K, Leetham T, Robertson W, Juels A, Kirda E. 2013. Beehive:\nlarge-scale log analysis for detecting suspicious activity in enterprise networks. In: Proceedings of\nthe 29th Annual Computer Security Applications Conference, New Orleans, Louisiana, USA.\nNew York: ACM, 199–208.\nCândido et al. (2021), PeerJ Comput. Sci., DOI 10.7717/peerj-cs.489\n37/38\n\n\nYoon E, Squicciarini A. 2014. Toward detecting compromised mapreduce workers through log\nanalysis. In: 2014 14th IEEE/ACM International Symposium on Cluster, Cloud and Grid\nComputing. Piscataway: IEEE, 41–50.\nYu X, Joshi P, Xu J, Jin G, Zhang H, Jiang G. 2016.",
    "section_title": "References",
    "section_level": 2,
    "page_number": 30,
    "chunk_index": 25,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "References",
      "section_level": 2
    }
  },
  {
    "text": "CloudSeer: workﬂow monitoring of cloud\ninfrastructures via interleaved logs. In: Proceedings of the Twenty-First International Conference\non Architectural Support for Programming Languages and Operating Systems, ASPLOS ’16,\nAtlanta, Georgia, USA. New York: ACM, 489–502.\nYuan D, Park S, Zhou Y. 2012. Characterizing logging practices in open-source software. In:\nProceedings of the 34th International Conference on Software Engineering, ICSE ’12, Zurich,\nSwitzerland. Piscataway: IEEE Press, 102–112.\nYuan D, Zheng J, Park S, Zhou Y, Savage S. 2012. Improving software diagnosability via log\nenhancement. ACM Transactions on Computer Systems 30(1):4:1–4:28.\nZeng Y, Chen J, Shang W, Chen T-HP. 2019. Studying the characteristics of logging practices in\nmobile apps: a case study on f-droid. Empirical Software Engineering 24(6):3394–3434.\nZhang X, Xu Y, Lin Q, Qiao B, Zhang H, Dang Y, Xie C, Yang X, Cheng Q, Li Z, Chen J, He X,\nYao R, Lou J-G, Chintalapati M, Shen F, Zhang D. 2019. Robust log-based anomaly detection\non unstable log data.",
    "section_title": "References",
    "section_level": 2,
    "page_number": 30,
    "chunk_index": 26,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "References",
      "section_level": 2
    }
  },
  {
    "text": "In: Proceedings of the 2019 27th ACM Joint Meeting on European Software\nEngineering Conference and Symposium on the Foundations of Software Engineering, Tallinn\nEstonia. New York: ACM, 807–817.\nZheng Z, Yu L, Tang W, Lan Z, Gupta R, Desai N, Coghlan S, Buettner D. 2011. Co-analysis of\nRAS log and job log on blue gene/P. In: 2011 IEEE International Parallel & Distributed\nProcessing Symposium, Anchorage, AK, USA. Piscataway: IEEE, 840–851.\nZhi C, Yin J, Deng S, Ye M, Fu M, Xie T. 2019. An exploratory study of logging conﬁguration\npractice in Java. In: 2019 IEEE International Conference on Software Maintenance and Evolution\n(ICSME). Piscataway: IEEE, 459–469.\nZhou P, Gill B, Belluomini W, Wildani A. 2010. GAUL: gestalt analysis of unstructured logs for\ndiagnosing recurring problems in large enterprise storage systems. In: 2010 29th IEEE\nSymposium on Reliable Distributed Systems, New Delhi, Punjab, India. Piscataway: IEEE,\n148–159.\nZhu J, He P, Fu Q, Zhang H, Lyu MR, Zhang D. 2015. Learning to log: helping developers make\ninformed logging decisions.",
    "section_title": "References",
    "section_level": 2,
    "page_number": 30,
    "chunk_index": 27,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "References",
      "section_level": 2
    }
  },
  {
    "text": "In: Proceedings of the 37th International Conference on Software\nEngineering - Volume 1, ICSE ’15, Florence, Italy. Piscataway: IEEE Press, 415–425.\nZhu J, He S, Liu J, He P, Xie Q, Zheng Z, Lyu MR. 2019. Tools and benchmarks for automated log\nparsing. In: 2019 IEEE/ACM 41st International Conference on Software Engineering: Software\nEngineering in Practice (ICSE-SEIP). Piscataway: IEEE, 121–130.\nCândido et al. (2021), PeerJ Comput. Sci., DOI 10.7717/peerj-cs.489\n38/38",
    "section_title": "References",
    "section_level": 2,
    "page_number": 30,
    "chunk_index": 28,
    "metadata": {
      "chunk_type": "section_part",
      "section_title": "References",
      "section_level": 2
    }
  }
]